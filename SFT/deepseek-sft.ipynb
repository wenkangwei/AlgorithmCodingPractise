{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa78c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40ff10dd",
   "metadata": {},
   "source": [
    "# Unsloth ÂÆòÁΩë\n",
    "https://docs.unsloth.ai/get-started/install-and-update/pip-install\n",
    "\n",
    "**ÁéØÂ¢É**\n",
    "- RTX5070\n",
    "- python3.13.9\n",
    "- torch-2.9.0 + cuda-toolkit-12.8\n",
    "- cuda-12.9\n",
    "\n",
    "**cuda-toolkit ÂÆâË£Ö**\n",
    "\n",
    "```shell\n",
    "conda create -n llm_env python==3.13.9\n",
    "conda activate llm_env\n",
    "conda install -c \"nvidia/label/cuda-12.8.0\" cuda-toolkit cuda-nvcc cuda-cudart cuda-driver-dev -y\n",
    "\n",
    "#Ê£ÄÊü•nvcc\n",
    "nvcc --version\n",
    "# Êü•ÁúãnvccÊîØÊåÅÁöÑgpuÊû∂ÊûÑÊòØÂê¶ÊîØÊåÅcompute_120Ôºå RTX5070 ÊòØÂíåcuda-12.8Êê≠ÈÖçÈúÄË¶ÅÊîØÊåÅcompute_120Êû∂ÊûÑ\n",
    "nvcc --list-gpu-arch \n",
    "\n",
    "### ËæìÂá∫\n",
    "#  compute_50\n",
    "# compute_52\n",
    "# compute_53\n",
    "# compute_60\n",
    "# compute_61\n",
    "# compute_62\n",
    "# compute_70\n",
    "# compute_72\n",
    "# compute_75\n",
    "# compute_80\n",
    "# compute_86\n",
    "# compute_87\n",
    "# compute_89\n",
    "# compute_90\n",
    "# compute_100\n",
    "# compute_101\n",
    "# compute_120\n",
    "```\n",
    "\n",
    "Áî±‰∫éÊòØÂú®conda ÁéØÂ¢ÉÂÆâË£Ö cuda-toolkit ÂÆâË£ÖÂà∞conda envÁöÑÁõÆÂΩï‰∏ãÈù¢Ôºå ÈúÄË¶ÅÊâãÂä®Êäälibcuda.so ËΩØËøûÊé•Âà∞/usr/lib ‰∏ãÈù¢Ôºå ÈúÄË¶Å‰∏ãÈù¢ÁöÑÈìæÊé•Êìç‰Ωú\n",
    "\n",
    "```shell\n",
    "# Ê£ÄÊü•Á≥ªÁªü CUDA Â∫ì‰ΩçÁΩÆ\n",
    "ldconfig -p | grep libcuda\n",
    "\n",
    "# ÂàõÂª∫Á¨¶Âè∑ÈìæÊé•Âà∞ conda ÁéØÂ¢É\n",
    "mkdir -p /home/wwk/workspace/miniconda3/envs/llm_study/lib64/stubs\n",
    "ln -s /usr/lib/x86_64-linux-gnu/libcuda.so /home/wwk/workspace/miniconda3/envs/llm_study/lib64/stubs/libcuda.so\n",
    "ln -s /usr/lib/x86_64-linux-gnu/libcuda.so.1 /home/wwk/workspace/miniconda3/envs/llm_study/lib64/stubs/libcuda.so.1\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**unsloth ÂÆâË£Ö**\n",
    "```shell\n",
    "pip install unsloth\n",
    "```\n",
    "\n",
    "**vllm ÂÆâË£Ö**\n",
    "```shell\n",
    "pip install vllm\n",
    "```\n",
    "\n",
    "\n",
    "**unsloth‰∏Ä‰∫õË∑ØÂæÑ**\n",
    "Ê®°ÂûãÈªòËÆ§‰∏ãËΩΩË∑ØÂæÑ(Âü∫Êú¨‰ΩøÁî®huggingfaceÁöÑË∑ØÂæÑ)Ôºö  ~/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c0d23c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import subprocess\n",
    "\n",
    "# # Check current CUDA availability\n",
    "# print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "# print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# # Find CUDA in conda environment\n",
    "# conda_prefix = os.environ.get('CONDA_PREFIX')\n",
    "# if conda_prefix:\n",
    "#     cuda_paths = [\n",
    "#         conda_prefix,\n",
    "#         os.path.join(conda_prefix, 'pkgs', 'cuda-toolkit'),\n",
    "#     ]\n",
    "    \n",
    "#     for path in cuda_paths:\n",
    "#         nvcc_path = os.path.join(path, 'bin', 'nvcc')\n",
    "#         if os.path.exists(nvcc_path):\n",
    "#             os.environ['CUDA_HOME'] = path\n",
    "#             os.environ['CUDA_ROOT'] = path\n",
    "#             os.environ['PATH'] = f\"{path}/bin:{os.environ['PATH']}\"\n",
    "#             os.environ['LD_LIBRARY_PATH'] = f\"{path}/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "#             print(f\"Set CUDA_HOME to: {path}\")\n",
    "#             break\n",
    "# conda_prefix = os.environ.get('CONDA_PREFIX')\n",
    "# if conda_prefix:\n",
    "#     print(f\"\\nSearching for CUDA in {conda_prefix}...\")\n",
    "#     for root, dirs, files in os.walk(conda_prefix):\n",
    "#         if 'nvcc' in files or 'libcudart.so' in files:\n",
    "#             print(f\"Found CUDA component in: {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c311cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1967330f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55ed52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU architecture: 12.0\n",
      "Cleared flashinfer cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import peft\n",
    "import trl\n",
    "\n",
    "# ËÆæÁΩÆÊîØÊåÅÁöÑ GPU Êû∂ÊûÑ\n",
    "# Ê†πÊçÆ‰Ω†ÁöÑ GPU ËÆæÁΩÆÊ≠£Á°ÆÁöÑÊû∂ÊûÑ\n",
    "os.environ[\"TORCH_CUDA_ARCH_LIST\"] = \"8.0;8.6;8.9;9.0\"  # Â∏∏ËßÅÊû∂ÊûÑ\n",
    "\n",
    "# ÊàñËÄÖËá™Âä®Ê£ÄÊµã\n",
    "if torch.cuda.is_available():\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    arch = f\"{capability[0]}.{capability[1]}\"\n",
    "    os.environ[\"TORCH_CUDA_ARCH_LIST\"] = arch\n",
    "    print(f\"Detected GPU architecture: {arch}\")\n",
    "\n",
    "## Â¶ÇÊûúunsloth Âá∫Áé∞flashinferÁõ∏ÂÖ≥Êä•ÈîôÔºå ÂèØ‰ª•ËØï‰∏Ä‰∏ãÊ∏ÖÈô§ flashinfer ÁºìÂ≠ò\n",
    "# import shutil\n",
    "# cache_dir = os.path.expanduser(\"~/.cache/flashinfer\")\n",
    "# if os.path.exists(cache_dir):\n",
    "#     shutil.rmtree(cache_dir)\n",
    "#     print(\"Cleared flashinfer cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b054f476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90857/477172518.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-29 22:38:40 [interface.py:409] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 11-29 22:38:47 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "==((====))==  Unsloth 2025.11.4: Fast Qwen3 patching. Transformers: 4.57.2. vLLM: 0.11.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070 Laptop GPU. Num GPUs = 1. Max memory: 7.96 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: b72fc53f-70e7-4ebb-8672-a1ad2135d18f)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/README.md\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: b72fc53f-70e7-4ebb-8672-a1ad2135d18f)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ffe46314-4bf7-4abd-8e94-ffd56339ae91)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/model.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ffe46314-4bf7-4abd-8e94-ffd56339ae91)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/model.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: e77f2eaf-fc57-4177-8649-a57db6a23c18)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/.gitattributes\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: e77f2eaf-fc57-4177-8649-a57db6a23c18)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/.gitattributes\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: b14a35db-2f1f-44d0-afe7-7e4020435529)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: b14a35db-2f1f-44d0-afe7-7e4020435529)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ae2782b6-3d97-4acc-bdba-aa453af2e980)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/README.md\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ae2782b6-3d97-4acc-bdba-aa453af2e980)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: cbceb14f-94f7-463d-bfd7-d310ef3b366c)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/model.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: cbceb14f-94f7-463d-bfd7-d310ef3b366c)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/model.safetensors\n",
      "Retrying in 2s [Retry 2/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 6e76f656-be9e-4d2b-8839-925375419b71)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/.gitattributes\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 6e76f656-be9e-4d2b-8839-925375419b71)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/.gitattributes\n",
      "Retrying in 2s [Retry 2/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 8324c04a-22c3-4d69-b383-657c383cd53c)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 8324c04a-22c3-4d69-b383-657c383cd53c)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: e8d8b12b-7fba-4a77-9abf-36ba827a3fb2)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/README.md\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: e8d8b12b-7fba-4a77-9abf-36ba827a3fb2)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 13221305-d680-4f16-bb7c-f197d5369271)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/model.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 13221305-d680-4f16-bb7c-f197d5369271)')' thrown while requesting HEAD https://huggingface.co/unslothai/other/resolve/43d9e0f2f19a5d7836895f648dc0e762816acf77/model.safetensors\n",
      "Retrying in 4s [Retry 3/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 50.65%\n",
      "Unsloth: Your GPU has CUDA compute capability 12.0 with VRAM = 7.96 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 1.28 GB. Also swap space = 0 GB.\n",
      "Unsloth: Not an error, but `use_cudagraph` is not supported in vLLM.config.CompilationConfig. Skipping.\n",
      "WARNING 11-29 22:39:22 [compilation.py:610] Level is deprecated and will be removed in the next release,either 0.12.0 or 0.11.2 whichever is soonest.Use mode instead.If both level and mode are given,only mode will be used.\n",
      "WARNING 11-29 22:39:22 [compilation.py:699] The 'use_inductor' flag is deprecated and will be removed in the next release (v0.12.0). Please use the 'backend' option instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 11-29 22:39:22 [utils.py:253] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 2048, 'enable_prefix_caching': True, 'swap_space': 0, 'gpu_memory_utilization': 0.506468111863445, 'max_num_batched_tokens': 2048, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': True, 'compile_sizes': None, 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 32, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'local_cache_dir': None}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
      "  return self.serializer.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:39:28 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-29 22:39:28 [model.py:1745] Using max model len 2048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 22:39:28,450\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:39:28 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 11-29 22:39:21 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': True, 'compile_sizes': [], 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 32, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 256, 'local_cache_dir': None}\n",
      "INFO 11-29 22:39:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.61.219:32987 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 11-29 22:39:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
      "  return self.serializer.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:39:22 [topk_topp_sampler.py:36] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-29 22:39:22 [gpu_model_runner.py:3259] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n",
      "INFO 11-29 22:39:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "INFO 11-29 22:39:22 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "INFO 11-29 22:39:23 [bitsandbytes_loader.py:791] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 11-29 22:39:26 [weight_utils.py:441] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 1.055983 seconds\n",
      "INFO 11-29 22:39:38 [weight_utils.py:481] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [-1:59:49<00:00, -0.09it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.33s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:39:31 [punica_selector.py:20] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:39:31 [gpu_model_runner.py:3338] Model loading took 3.2439 GiB memory and 8.725790 seconds\n",
      "INFO 11-29 22:39:54 [backends.py:631] Using cache directory: /home/wwk/.cache/vllm/torch_compile_cache/2b0661a394/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 11-29 22:39:54 [backends.py:647] Dynamo bytecode transform time: 10.66 s\n",
      "INFO 11-29 22:39:57 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.783 s\n",
      "INFO 11-29 22:39:59 [monitor.py:34] torch.compile takes 13.45 s in total\n",
      "INFO 11-29 22:40:33 [gpu_worker.py:359] Available KV cache memory: 0.50 GiB\n",
      "INFO 11-29 22:40:34 [kv_cache_utils.py:1229] GPU KV cache size: 3,648 tokens\n",
      "INFO 11-29 22:40:34 [kv_cache_utils.py:1234] Maximum concurrency for 2,048 tokens per request: 1.78x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 22:40:34,138 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-29 22:40:34,515 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:40:34 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-29 22:40:35 [utils.py:250] Using default LoRA kernel configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:02<00:00, 27.34it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:18<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:40:55 [gpu_model_runner.py:4244] Graph capturing finished in 21 secs, took 1.93 GiB\n",
      "INFO 11-29 22:40:55 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 21 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 22:40:56 [core.py:250] init engine (profile, create kv cache, warmup model) took 85.01 seconds\n",
      "INFO 11-29 22:41:01 [llm.py:352] Supported tasks: ('generate',)\n",
      "Unsloth: Just some info: will skip parsing ['attention_norm', 'ffn_norm', 'norm1', 'layer_norm1', 'post_feedforward_layernorm', 'input_layernorm', 'layer_norm2', 'norm2', 'post_layernorm', 'q_norm', 'k_norm', 'norm', 'pre_feedforward_layernorm', 'post_attention_layernorm']\n",
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['cross_attn_post_attention_layernorm', 'attention_norm', 'ffn_norm', 'norm1', 'layer_norm1', 'post_feedforward_layernorm', 'input_layernorm', 'layer_norm2', 'cross_attn_input_layernorm', 'norm2', 'post_layernorm', 'q_norm', 'k_norm', 'norm', 'pre_feedforward_layernorm', 'post_attention_layernorm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.4 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.6 # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92797c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "843439e6",
   "metadata": {},
   "source": [
    "# Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65751596",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "# system_prompt\n",
    "\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with out specific template:\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab3137b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] + eos_token }}{% set loop_messages = messages[1:] %}{% else %}{{ 'You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "Place it between <start_working_out> and <end_working_out>.\n",
      "Then, provide your solution between <SOLUTION></SOLUTION>' + eos_token }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ message['content'] }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<start_working_out>' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.chat_template = chat_template\n",
    "print(tokenizer.chat_template) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194bd6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>What is 1+1?<start_working_out>I think it's 2.<end_working_out><SOLUTION>2</SOLUTION><|endoftext|>What is 2+2?<start_working_out>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template([\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
    "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
    "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
    "], tokenize = False, add_generation_prompt = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079cc87",
   "metadata": {},
   "source": [
    "# Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "826e1999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_answer</th>\n",
       "      <th>problem</th>\n",
       "      <th>generated_solution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to solve the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-2</td>\n",
       "      <td>Find the value of the parameter $a$ for which ...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>What is the sum of all real numbers $x$ for wh...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to solve the equation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to evaluate the infin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>30</td>\n",
       "      <td>What is the largest positive integer that divi...</td>\n",
       "      <td>&lt;think&gt;\\nAlright, so I need to find the larges...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19243</th>\n",
       "      <td>244</td>\n",
       "      <td>Let \\( p \\), \\( q \\), and \\( r \\) be the disti...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19245</th>\n",
       "      <td>1</td>\n",
       "      <td>A bug is on the $0$ of a number line. At any p...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I have this problem where a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19247</th>\n",
       "      <td>4</td>\n",
       "      <td>A bus left point X for point Y. Two hours late...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19248</th>\n",
       "      <td>18</td>\n",
       "      <td>Each interior angle of a regular n-gon measure...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19250</th>\n",
       "      <td>0.8960</td>\n",
       "      <td>Find the probability that the second blue resu...</td>\n",
       "      <td>&lt;think&gt;\\nOkay, so I need to find the probabili...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7507 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      expected_answer                                            problem  \\\n",
       "0                  14  Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...   \n",
       "6                  -2  Find the value of the parameter $a$ for which ...   \n",
       "9                  18  What is the sum of all real numbers $x$ for wh...   \n",
       "13                  2  Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...   \n",
       "17                 30  What is the largest positive integer that divi...   \n",
       "...               ...                                                ...   \n",
       "19243             244  Let \\( p \\), \\( q \\), and \\( r \\) be the disti...   \n",
       "19245               1  A bug is on the $0$ of a number line. At any p...   \n",
       "19247               4  A bus left point X for point Y. Two hours late...   \n",
       "19248              18  Each interior angle of a regular n-gon measure...   \n",
       "19250          0.8960  Find the probability that the second blue resu...   \n",
       "\n",
       "                                      generated_solution  \n",
       "0      <think>\\nOkay, let's see. I need to solve the ...  \n",
       "6      <think>\\nOkay, so I need to find the value of ...  \n",
       "9      <think>\\nOkay, so I need to solve the equation...  \n",
       "13     <think>\\nOkay, so I need to evaluate the infin...  \n",
       "17     <think>\\nAlright, so I need to find the larges...  \n",
       "...                                                  ...  \n",
       "19243  <think>\\nOkay, so I need to find the value of ...  \n",
       "19245  <think>\\nOkay, so I have this problem where a ...  \n",
       "19247  <think>\\nOkay, let's tackle this problem step ...  \n",
       "19248  <think>\\nOkay, let's see. I need to find the n...  \n",
       "19250  <think>\\nOkay, so I need to find the probabili...  \n",
       "\n",
       "[7507 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
    "dataset = dataset.to_pandas()[\n",
    "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
    "]\n",
    "\n",
    "# Try converting to number - if not, replace with NaN\n",
    "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
    "# Select only numbers\n",
    "dataset = dataset.iloc[np.where(is_number)[0]]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3947bd",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baffdbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<start_working_out>Okay, let's see. I need to solve the equation ‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n‚àö(x¬≤ + 165) = 7 + ‚àö(x¬≤ - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(‚àö(x¬≤ + 165))¬≤ = (7 + ‚àö(x¬≤ - 52))¬≤.\\n\\nSimplifying the left side:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤.\\n\\nThe right side is expanded using the formula (a + b)¬≤ = a¬≤ + 2ab + b¬≤. So the right side becomes 7¬≤ + 2*7*‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤, which is 49 + 14‚àö(x¬≤ - 52) + (x¬≤ - 52).\\n\\nSo putting it all together:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + x¬≤ - 52.\\n\\nHmm, let's simplify the right side. The x¬≤ terms will cancel out, right? Let's subtract x¬≤ from both sides:\\n\\n165 = 49 + 14‚àö(x¬≤ - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14‚àö(x¬≤ - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14‚àö(x¬≤ - 52).\\n\\nSo 168 = 14‚àö(x¬≤ - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = ‚àö(x¬≤ - 52).\\n\\n12 = ‚àö(x¬≤ - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12¬≤ = x¬≤ - 52.\\n\\n144 = x¬≤ - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x¬≤.\\n\\n196 = x¬≤.\\n\\nSo x = ‚àö196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n‚àö(14¬≤ + 165) - ‚àö(14¬≤ - 52) = ?\\n\\nCalculate each term:\\n\\n14¬≤ is 196.\\n\\nSo first radical: ‚àö(196 + 165) = ‚àö361 = 19.\\n\\nSecond radical: ‚àö(196 - 52) = ‚àö144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<end_working_out><SOLUTION>14</SOLUTION><|endoftext|>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def format_dataset(x):\n",
    "    expected_answer = x[\"expected_answer\"]\n",
    "    problem = x[\"problem\"]\n",
    "\n",
    "    # Remove generated <think> and </think>\n",
    "    thoughts = x[\"generated_solution\"]\n",
    "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "\n",
    "    # Strip newlines on left and right\n",
    "    thoughts = thoughts.strip()\n",
    "    # Add our custom formatting\n",
    "    final_prompt = \\\n",
    "        reasoning_start + thoughts + reasoning_end + \\\n",
    "        solution_start + expected_answer + solution_end\n",
    "    # system_prompt = role\n",
    "    # problem\n",
    "    # final_prompt = CoT + answer\n",
    "    return [\n",
    "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
    "        {\"role\" : \"user\",      \"content\" : problem},\n",
    "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
    "    ]\n",
    "\n",
    "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)\n",
    "tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b162d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['expected_answer', 'problem', 'generated_solution', 'Messages', 'N', 'text', '__index_level_0__'],\n",
       "    num_rows: 59\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n",
    "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n",
    "print(dataset.shape)\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db37529c",
   "metadata": {},
   "source": [
    "# SFT Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00784b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5070 Laptop GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=36): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59/59 [00:08<00:00,  6.72 examples/s]\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 59 | Num Epochs = 2 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 2 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:01, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.499300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.466800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.406500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.291100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.275100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.39601008892059325, metrics={'train_runtime': 136.0745, 'train_samples_per_second': 0.867, 'train_steps_per_second': 0.441, 'total_flos': 1353455198164992.0, 'train_loss': 0.39601008892059325, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 1,\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        gradient_accumulation_steps = 2, # Use GA to mimic batch size!\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 5,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        # Ê∑ªÂä†ÂÜÖÂ≠òÁõ∏ÂÖ≥ÂèÇÊï∞\n",
    "        dataloader_pin_memory=False,\n",
    "        remove_unused_columns=False,\n",
    "        max_seq_length=1024,  # Ê∑ªÂä†Â∫èÂàóÈïøÂ∫¶\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19988564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PeftModelForCausalLM in module peft.peft_model object:\n",
      "\n",
      "class PeftModelForCausalLM(PeftModel)\n",
      " |  PeftModelForCausalLM(\n",
      " |      model: 'torch.nn.Module',\n",
      " |      peft_config: 'PeftConfig',\n",
      " |      adapter_name: 'str' = 'default',\n",
      " |      **kwargs\n",
      " |  ) -> 'None'\n",
      " |\n",
      " |  Peft model for causal language modeling.\n",
      " |\n",
      " |  Args:\n",
      " |      model ([`~transformers.PreTrainedModel`]): Base transformer model.\n",
      " |      peft_config ([`PeftConfig`]): Peft config.\n",
      " |      adapter_name (`str`,  *optional*): The name of the adapter, defaults to `\"default\"`.\n",
      " |      autocast_adapter_dtype (`bool`, *optional*):\n",
      " |          Whether to autocast the adapter dtype. Defaults to `True`. Right now, this will only cast adapter weights\n",
      " |          using float16 and bfloat16 to float32, as this is typically required for stable training, and only affect\n",
      " |          select PEFT tuners.\n",
      " |\n",
      " |  Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import AutoModelForCausalLM\n",
      " |      >>> from peft import PeftModelForCausalLM, get_peft_config\n",
      " |\n",
      " |      >>> config = {\n",
      " |      ...     \"peft_type\": \"PREFIX_TUNING\",\n",
      " |      ...     \"task_type\": \"CAUSAL_LM\",\n",
      " |      ...     \"inference_mode\": False,\n",
      " |      ...     \"num_virtual_tokens\": 20,\n",
      " |      ...     \"token_dim\": 1280,\n",
      " |      ...     \"num_transformer_submodules\": 1,\n",
      " |      ...     \"num_attention_heads\": 20,\n",
      " |      ...     \"num_layers\": 36,\n",
      " |      ...     \"encoder_hidden_size\": 1280,\n",
      " |      ...     \"prefix_projection\": False,\n",
      " |      ...     \"postprocess_past_key_value_function\": None,\n",
      " |      ... }\n",
      " |\n",
      " |      >>> peft_config = get_peft_config(config)\n",
      " |      >>> model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\")\n",
      " |      >>> peft_model = PeftModelForCausalLM(model, peft_config)\n",
      " |      >>> peft_model.print_trainable_parameters()\n",
      " |      trainable params: 1843200 || all params: 775873280 || trainable%: 0.23756456724479544\n",
      " |      ```\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      PeftModelForCausalLM\n",
      " |      PeftModel\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      model: 'torch.nn.Module',\n",
      " |      peft_config: 'PeftConfig',\n",
      " |      adapter_name: 'str' = 'default',\n",
      " |      **kwargs\n",
      " |  ) -> 'None'\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  forward = PeftModel_fast_forward(\n",
      " |      self,\n",
      " |      input_ids=None,\n",
      " |      causal_mask=None,\n",
      " |      attention_mask=None,\n",
      " |      inputs_embeds=None,\n",
      " |      labels=None,\n",
      " |      output_attentions=None,\n",
      " |      output_hidden_states=None,\n",
      " |      return_dict=None,\n",
      " |      task_ids=None,\n",
      " |      num_logits_to_keep=0,\n",
      " |      logits_to_keep=0,\n",
      " |      **kwargs\n",
      " |  ) from unsloth.models.llama\n",
      " |\n",
      " |  generate(self, *args, **kwargs)\n",
      " |\n",
      " |  prepare_inputs_for_generation(\n",
      " |      self,\n",
      " |      *args,\n",
      " |      task_ids: 'Optional[torch.Tensor]' = None,\n",
      " |      **kwargs\n",
      " |  )\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from PeftModel:\n",
      " |\n",
      " |  __getattr__(self, name: 'str')\n",
      " |      Forward missing attributes to the wrapped module.\n",
      " |\n",
      " |  add_adapter(\n",
      " |      self,\n",
      " |      adapter_name: 'str',\n",
      " |      peft_config: 'PeftConfig',\n",
      " |      low_cpu_mem_usage: 'bool' = False\n",
      " |  ) -> 'None'\n",
      " |      Add an adapter to the model based on the passed configuration.\n",
      " |\n",
      " |      This adapter is not trained. To load a trained adapter, check out [`PeftModel.load_adapter`].\n",
      " |\n",
      " |      The name for the new adapter should be unique.\n",
      " |\n",
      " |      The new adapter is not automatically set as the active adapter. Use [`PeftModel.set_adapter`] to set the active\n",
      " |      adapter.\n",
      " |\n",
      " |      Args:\n",
      " |          adapter_name (`str`):\n",
      " |              The name of the adapter to be added.\n",
      " |          peft_config ([`PeftConfig`]):\n",
      " |              The configuration of the adapter to be added.\n",
      " |          low_cpu_mem_usage (`bool`, `optional`, defaults to `False`):\n",
      " |              Create empty adapter weights on meta device. Useful to speed up the process when loading saved\n",
      " |              adapters. Don't use this option when creating a new PEFT adapter for training.\n",
      " |\n",
      " |  create_or_update_model_card(self, output_dir: 'str')\n",
      " |      Updates or create model card to include information about peft:\n",
      " |      1. Adds `peft` library tag\n",
      " |      2. Adds peft version\n",
      " |      3. Adds base model info\n",
      " |      4. Adds quantization information if it was used\n",
      " |\n",
      " |  delete_adapter(self, adapter_name: 'str') -> 'None'\n",
      " |      Deletes an existing adapter.\n",
      " |\n",
      " |      Args:\n",
      " |          adapter_name (str): Name of the adapter to be deleted.\n",
      " |\n",
      " |  disable_adapter(self)\n",
      " |      Context manager that disables the adapter module. Use this to run inference on the base model.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> with model.disable_adapter():\n",
      " |      ...     model(inputs)\n",
      " |      ```\n",
      " |\n",
      " |  get_base_model(self) -> 'torch.nn.Module'\n",
      " |      Returns the base model.\n",
      " |\n",
      " |  get_layer_status(self) -> 'list[TunerLayerStatus]'\n",
      " |      Get the status of each adapter layer in the model.\n",
      " |\n",
      " |      This method returns a list of `TunerLayerStatus` dataclass instances, each of which contains the following\n",
      " |      attributes:\n",
      " |\n",
      " |      - `name` (`str`):\n",
      " |         The name of the adapter layer, e.g. `model.encoder.block.0.layer.0.SelfAttention.q`.\n",
      " |      - `module_type` (`str`):\n",
      " |         The type of the adapter layer, e.g. `lora.Linear`.\n",
      " |      - `enabled` (`bool`):\n",
      " |         Whether the adapter layer is enabled.\n",
      " |      - `active_adapters` (`list[str]`):\n",
      " |         The names of the active adapters, if any, e.g. `[\"default\"]`.\n",
      " |      - `merged_adapters` (`list[str]`):\n",
      " |         The names of the merged adapters, if any, e.g. `[\"default\"]`.\n",
      " |      - `available_adapters` (`list[str]`):\n",
      " |         The names of the available adapters, e.g. `[\"default\"]`.\n",
      " |\n",
      " |      Args:\n",
      " |          model ([`~PeftModel`]):\n",
      " |              The model to get the adapter layer status from.\n",
      " |\n",
      " |      Returns:\n",
      " |          list[`peft.peft_model.TunerLayerStatus`]:\n",
      " |              A list of dataclasses, each containing the status of the corresponding adapter layer.\n",
      " |\n",
      " |  get_model_status(self) -> 'TunerModelStatus'\n",
      " |      Get the status of tuners of the model.\n",
      " |\n",
      " |      This method returns a `TunerModelStatus` dataclass instance, which contains the following attributes:\n",
      " |\n",
      " |      - `base_model_type` (`str`):\n",
      " |         The type of the base model, e.g. `T5Model`.\n",
      " |      - `adapter_model_type` (`str`):\n",
      " |         The type of the adapter model, e.g. `LoraModel`.\n",
      " |      - `peft_types` (`dict[str, str]`):\n",
      " |         The mapping of adapter name to adapter type, e.g. `{\"default\": \"LORA\"}`.\n",
      " |      - `trainable_params` (`int`):\n",
      " |         The number of trainable parameters in the model.\n",
      " |      - `total_params` (`int`):\n",
      " |         The total number of parameters in the model.\n",
      " |      - `num_adapter_layers` (`int`):\n",
      " |         The number of adapter layers in the model.\n",
      " |      - `enabled` (`bool`, `Literal[\"irregular\"]`):\n",
      " |         Whether all adapter layers are enabled. If some are enabled and some are not, this will be `\"irregular\"`.\n",
      " |         This means that your model is in an inconsistent state and might not work as expected.\n",
      " |      - `active_adapters` (`list[str]`, `Literal[\"irregular\"]`):\n",
      " |         The names of the active adapters. If the active adapters are not consistent across all layers, this will be\n",
      " |         `\"irregular\"`, which means that your model is in an inconsistent state and might not work as expected.\n",
      " |      - `merged_adapters` (`list[str]`, `Literal[\"irregular\"]`):\n",
      " |         The names of the merged adapters. If the merged adapters are not consistent across all layers, this will be\n",
      " |         `\"irregular\"`, which means that your model is in an inconsistent state and might not work as expected.\n",
      " |      - `available_adapters` (`list[str]`):\n",
      " |         The names of the available adapters, e.g. `[\"default\"]`.\n",
      " |\n",
      " |      Args:\n",
      " |          model ([`~PeftModel`]):\n",
      " |              The model to get the adapter layer status from.\n",
      " |\n",
      " |      Returns:\n",
      " |          `peft.peft_model.TunerModelStatus`:\n",
      " |              A dataclass containing the status of the model.\n",
      " |\n",
      " |  get_nb_trainable_parameters(self) -> 'tuple[int, int]'\n",
      " |      Returns the number of trainable parameters and the number of all parameters in the model.\n",
      " |\n",
      " |  get_prompt(\n",
      " |      self,\n",
      " |      batch_size: 'int',\n",
      " |      task_ids: 'Optional[torch.Tensor]' = None,\n",
      " |      max_cache_len: 'Optional[int]' = None\n",
      " |  ) -> 'torch.Tensor'\n",
      " |      Returns the virtual prompts to use for Peft. Only applicable when using a prompt learning method.\n",
      " |\n",
      " |  get_prompt_embedding_to_save(self, adapter_name: 'str') -> 'torch.Tensor'\n",
      " |      Returns the prompt embedding to save when saving the model. Only applicable when using a prompt learning\n",
      " |      method.\n",
      " |\n",
      " |  load_adapter(\n",
      " |      self,\n",
      " |      model_id: 'Union[str, os.PathLike]',\n",
      " |      adapter_name: 'str',\n",
      " |      is_trainable: 'bool' = False,\n",
      " |      torch_device: 'Optional[str]' = None,\n",
      " |      autocast_adapter_dtype: 'bool' = True,\n",
      " |      ephemeral_gpu_offload: 'bool' = False,\n",
      " |      low_cpu_mem_usage: 'bool' = False,\n",
      " |      key_mapping: 'Optional[dict[str, str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  )\n",
      " |      Load a trained adapter into the model.\n",
      " |\n",
      " |      The name for the new adapter should be unique.\n",
      " |\n",
      " |      The new adapter is not automatically set as the active adapter. Use [`PeftModel.set_adapter`] to set the active\n",
      " |      adapter.\n",
      " |\n",
      " |      Args:\n",
      " |          model_id (`str` or `os.PathLike`):\n",
      " |              The name of the PEFT configuration to use. Can be either:\n",
      " |                  - A string, the `model id` of a PEFT configuration hosted inside a model repo on the Hugging Face\n",
      " |                    Hub.\n",
      " |                  - A path to a directory containing a PEFT configuration file saved using the `save_pretrained`\n",
      " |                    method (`./my_peft_config_directory/`).\n",
      " |          adapter_name (`str`):\n",
      " |              The name of the adapter to be added.\n",
      " |          is_trainable (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n",
      " |              used for inference.\n",
      " |          torch_device (`str`, *optional*, defaults to None):\n",
      " |              The device to load the adapter on. If `None`, the device will be inferred.\n",
      " |          autocast_adapter_dtype (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to autocast the adapter dtype. Defaults to `True`. Right now, this will only cast adapter\n",
      " |              weights using float16 and bfloat16 to float32, as this is typically required for stable training, and\n",
      " |              only affect select PEFT tuners.\n",
      " |          ephemeral_gpu_offload (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to use ephemeral GPU offloading for partially loaded modules. Defaults to `False`.\n",
      " |          low_cpu_mem_usage (`bool`, `optional`, defaults to `False`):\n",
      " |              Create empty adapter weights on meta device before loading the saved weights. Useful to speed up the\n",
      " |              process.\n",
      " |          key_mapping (dict, *optional*, defaults to None)\n",
      " |              Extra mapping of PEFT `state_dict` keys applied before loading the `state_dict`. When this mapping is\n",
      " |              applied, the PEFT-specific `\"base_model.model\"` prefix is removed beforehand and the adapter name (e.g.\n",
      " |              `\"default\"`) is not inserted yet. Only pass this argument if you know what you're doing.\n",
      " |          kwargs: (`optional`):\n",
      " |              Additional arguments to modify the way the adapter is loaded, e.g. the token for Hugging Face Hub.\n",
      " |\n",
      " |  prepare_model_for_gradient_checkpointing(self, model: 'PreTrainedModel')\n",
      " |      Prepares the model for gradient checkpointing if necessary\n",
      " |\n",
      " |  print_trainable_parameters(self) -> 'None'\n",
      " |      Prints the number of trainable parameters in the model.\n",
      " |\n",
      " |      Note: print_trainable_parameters() uses get_nb_trainable_parameters() which is different from\n",
      " |      num_parameters(only_trainable=True) from huggingface/transformers. get_nb_trainable_parameters() returns\n",
      " |      (trainable parameters, all parameters) of the Peft Model which includes modified backbone transformer model.\n",
      " |      For techniques like LoRA, the backbone transformer model is modified in place with LoRA modules. However, for\n",
      " |      prompt tuning, the backbone transformer model is unmodified. num_parameters(only_trainable=True) returns number\n",
      " |      of trainable parameters of the backbone transformer model which can be different.\n",
      " |\n",
      " |  save_pretrained(\n",
      " |      self,\n",
      " |      save_directory: 'str',\n",
      " |      safe_serialization: 'bool' = True,\n",
      " |      selected_adapters: 'Optional[list[str]]' = None,\n",
      " |      save_embedding_layers: 'Union[str, bool]' = 'auto',\n",
      " |      is_main_process: 'bool' = True,\n",
      " |      path_initial_model_for_weight_conversion: 'Optional[str]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'None'\n",
      " |      This function saves the adapter model and the adapter configuration files to a directory, so that it can be\n",
      " |      reloaded using the [`PeftModel.from_pretrained`] class method, and also used by the [`PeftModel.push_to_hub`]\n",
      " |      method.\n",
      " |\n",
      " |      Args:\n",
      " |          save_directory (`str`):\n",
      " |              Directory where the adapter model and configuration files will be saved (will be created if it does not\n",
      " |              exist).\n",
      " |          safe_serialization (`bool`, *optional*):\n",
      " |              Whether to save the adapter files in safetensors format, defaults to `True`.\n",
      " |          selected_adapters (`List[str]`,  *optional*):\n",
      " |              A list of adapters to be saved. If `None`, will default to all adapters.\n",
      " |          save_embedding_layers (`Union[bool, str]`, *optional*, defaults to `\"auto\"`):\n",
      " |              If `True`, save the embedding layers in addition to adapter weights. If `auto`, checks the common\n",
      " |              embedding layers `peft.utils.other.EMBEDDING_LAYER_NAMES` in config's `target_modules` when available.\n",
      " |              and automatically sets the boolean flag. This only works for ü§ó transformers models.\n",
      " |          is_main_process (`bool`, *optional*):\n",
      " |              Whether the process calling this is the main process or not. Will default to `True`. Will not save the\n",
      " |              checkpoint if not on the main process, which is important for multi device setups (e.g. DDP).\n",
      " |          path_initial_model_for_weight_conversion (`str, *optional*`):\n",
      " |              The path to the initialized adapter, which is obtained after initializing the model with\n",
      " |              PiSSA/CorDA/OLoRA and before performing any training. When `path_initial_model_for_weight_conversion`\n",
      " |              is not None, the difference in adapter before and after fine-tuning is calculated. This difference can\n",
      " |              be represented as the parameters of a standard LoRA adapter. Using this converted adapter does not\n",
      " |              require changes to the base model, thus conveniently allowing the use of multiple PiSSA/CorDA/OLoRA\n",
      " |              adapters with LoRA adapters, and the activation or deactivation of any adapters. Note that this\n",
      " |              conversion is not supported if `rslora` is used in combination with `rank_pattern` or `alpha_pattern`.\n",
      " |          kwargs (additional keyword arguments, *optional*):\n",
      " |              Additional keyword arguments passed along to the `push_to_hub` method.\n",
      " |\n",
      " |  set_adapter(self, adapter_name: 'str') -> 'None'\n",
      " |      Sets the active adapter.\n",
      " |\n",
      " |      Only one adapter can be active at a time.\n",
      " |\n",
      " |      Additionally, this function will set the specified adapter to trainable (i.e., requires_grad=True). If this is\n",
      " |      not desired, use the following code.\n",
      " |\n",
      " |      ```py\n",
      " |      >>> for name, param in model_peft.named_parameters():\n",
      " |      ...     if ...:  # some check on name (ex. if 'lora' in name)\n",
      " |      ...         param.requires_grad = False\n",
      " |      ```\n",
      " |\n",
      " |      Args:\n",
      " |          adapter_name (`str`):\n",
      " |              The name of the adapter to be set as active. The adapter must be loaded first.\n",
      " |\n",
      " |  set_requires_grad(\n",
      " |      self,\n",
      " |      adapter_names: 'str | Sequence[str]',\n",
      " |      requires_grad: 'bool' = True\n",
      " |  ) -> 'None'\n",
      " |      Enable or disable gradients on the given adapter(s).\n",
      " |\n",
      " |      Note: Not supported for prompt learning methods like prompt tuning.\n",
      " |\n",
      " |      Args:\n",
      " |          adapter_name (`str` or `Sequence[str]`):\n",
      " |              The name of the adapter(s) whose gradients should be enabled/disabled.\n",
      " |          requires_grad (`bool`, *optional*)\n",
      " |              Whether to enable (`True`, default) or disable (`False`).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from PeftModel:\n",
      " |\n",
      " |  from_pretrained(\n",
      " |      model: 'torch.nn.Module',\n",
      " |      model_id: 'Union[str, os.PathLike]',\n",
      " |      adapter_name: 'str' = 'default',\n",
      " |      is_trainable: 'bool' = False,\n",
      " |      config: 'Optional[PeftConfig]' = None,\n",
      " |      autocast_adapter_dtype: 'bool' = True,\n",
      " |      ephemeral_gpu_offload: 'bool' = False,\n",
      " |      low_cpu_mem_usage: 'bool' = False,\n",
      " |      key_mapping: 'Optional[dict[str, str]]' = None,\n",
      " |      **kwargs: 'Any'\n",
      " |  ) -> 'PeftModel'\n",
      " |      Instantiate a PEFT model from a pretrained model and loaded PEFT weights.\n",
      " |\n",
      " |      Note that the passed `model` may be modified inplace.\n",
      " |\n",
      " |      Args:\n",
      " |          model ([`torch.nn.Module`]):\n",
      " |              The model to be adapted. For ü§ó Transformers models, the model should be initialized with the\n",
      " |              [`~transformers.PreTrainedModel.from_pretrained`].\n",
      " |          model_id (`str` or `os.PathLike`):\n",
      " |              The name of the PEFT configuration to use. Can be either:\n",
      " |                  - A string, the `model id` of a PEFT configuration hosted inside a model repo on the Hugging Face\n",
      " |                    Hub.\n",
      " |                  - A path to a directory containing a PEFT configuration file saved using the `save_pretrained`\n",
      " |                    method (`./my_peft_config_directory/`).\n",
      " |          adapter_name (`str`, *optional*, defaults to `\"default\"`):\n",
      " |              The name of the adapter to be loaded. This is useful for loading multiple adapters.\n",
      " |          is_trainable (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n",
      " |              used for inference.\n",
      " |          config ([`~peft.PeftConfig`], *optional*):\n",
      " |              The configuration object to use instead of an automatically loaded configuration. This configuration\n",
      " |              object is mutually exclusive with `model_id` and `kwargs`. This is useful when configuration is already\n",
      " |              loaded before calling `from_pretrained`.\n",
      " |          autocast_adapter_dtype (`bool`, *optional*):\n",
      " |              Whether to autocast the adapter dtype. Defaults to `True`. Only relevant for specific adapter types.\n",
      " |          ephemeral_gpu_offload (`bool`, *optional*):\n",
      " |              Whether to use ephemeral GPU offloading for partially loaded modules. Defaults to `False`. This is\n",
      " |              useful when parts of the model and/or components (such as adapters) are kept in CPU memory until they\n",
      " |              are needed. Rather than perform expensive operations on small data, the data is transferred to the GPU\n",
      " |              on-demand, the operation(s) performed, and the results moved back to CPU memory. This brings a slight\n",
      " |              momentary VRAM overhead but gives orders of magnitude speedup in certain cases.\n",
      " |          low_cpu_mem_usage (`bool`, `optional`, defaults to `False`):\n",
      " |              Create empty adapter weights on meta device before loading the saved weights. Useful to speed up the\n",
      " |              process.\n",
      " |          torch_device (`str`, *optional*, defaults to None):\n",
      " |              The device to load the adapter on. If `None`, the device will be inferred.\n",
      " |          key_mapping (dict, *optional*, defaults to None)\n",
      " |              Extra mapping of PEFT `state_dict` keys applied before loading the `state_dict`. When this mapping is\n",
      " |              applied, the PEFT-specific `\"base_model.model\"` prefix is removed beforehand and the adapter name (e.g.\n",
      " |              `\"default\"`) is not inserted yet. Only pass this argument if you know what you're doing.\n",
      " |          kwargs: (`optional`):\n",
      " |              Additional keyword arguments passed along to the specific PEFT configuration class.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from PeftModel:\n",
      " |\n",
      " |  active_adapters\n",
      " |\n",
      " |  active_peft_config\n",
      " |\n",
      " |  base_model_torch_dtype\n",
      " |\n",
      " |  has_active_enabled_adapter\n",
      " |      Reflects whether the adapters are purposefully disabled (via disable_adapter) or if there\n",
      " |      are no active adapters (enabled but inactive). They are two separate mechanisms but sometimes it is helpful to\n",
      " |      know whether the model has any active/enabled adapter at all.\n",
      " |\n",
      " |  modules_to_save\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from PeftModel:\n",
      " |\n",
      " |  peft_config\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.utils.hub.PushToHubMixin:\n",
      " |\n",
      " |  push_to_hub(\n",
      " |      self,\n",
      " |      repo_id: str,\n",
      " |      use_temp_dir: Optional[bool] = None,\n",
      " |      commit_message: Optional[str] = None,\n",
      " |      private: Optional[bool] = None,\n",
      " |      token: Union[bool, str, NoneType] = None,\n",
      " |      max_shard_size: Union[str, int, NoneType] = '5GB',\n",
      " |      create_pr: bool = False,\n",
      " |      safe_serialization: bool = True,\n",
      " |      revision: Optional[str] = None,\n",
      " |      commit_description: Optional[str] = None,\n",
      " |      tags: Optional[list[str]] = None,\n",
      " |      **deprecated_kwargs\n",
      " |  ) -> str\n",
      " |      Upload the {object_files} to the ü§ó Model Hub.\n",
      " |\n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your {object} to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload {object}\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`list[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      from transformers import {object_class}\n",
      " |\n",
      " |      {object} = {object_class}.from_pretrained(\"google-bert/bert-base-cased\")\n",
      " |\n",
      " |      # Push the {object} to your namespace with the name \"my-finetuned-bert\".\n",
      " |      {object}.push_to_hub(\"my-finetuned-bert\")\n",
      " |\n",
      " |      # Push the {object} to an organization with the name \"my-finetuned-bert\".\n",
      " |      {object}.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.utils.hub.PushToHubMixin:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |\n",
      " |  __delattr__(self, name) -> None\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |\n",
      " |  __setstate__ = patched_setstate(self: torch.nn.modules.module.Module, state: Any) -> None from torch._dynamo.mutation_guard.install_generation_tagging_init.<locals>\n",
      " |\n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |\n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |\n",
      " |  apply(self, fn: Callable[[ForwardRef('Module')], NoneType]) -> Self\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |\n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |\n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |\n",
      " |  bfloat16(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  buffers(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |\n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |\n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |\n",
      " |  children(self) -> collections.abc.Iterator['Module']\n",
      " |      Return an iterator over immediate children modules.\n",
      " |\n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |\n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |\n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |\n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |\n",
      " |  cpu(self) -> Self\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  cuda(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing the optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  double(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  eval(self) -> Self\n",
      " |      Set the module in evaluation mode.\n",
      " |\n",
      " |      This has an effect only on certain modules. See the documentation of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |\n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |\n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  extra_repr(self) -> str\n",
      " |      Return the extra representation of the module.\n",
      " |\n",
      " |      To print customized extra information, you should re-implement\n",
      " |      this method in your own modules. Both single-line and multi-line\n",
      " |      strings are acceptable.\n",
      " |\n",
      " |  float(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |\n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |\n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |\n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |\n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |\n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |\n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |\n",
      " |      .. code-block:: text\n",
      " |\n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |\n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |\n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |\n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |\n",
      " |      Raises:\n",
      " |          AttributeError: If at any point along the path resulting from\n",
      " |              the target string the (sub)path resolves to a non-existent\n",
      " |              attribute name or an object that is not an instance of ``nn.Module``.\n",
      " |\n",
      " |  half(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  ipu(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing the optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  load_state_dict(\n",
      " |      self,\n",
      " |      state_dict: collections.abc.Mapping[str, typing.Any],\n",
      " |      strict: bool = True,\n",
      " |      assign: bool = False\n",
      " |  )\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |\n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |\n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict` unless\n",
      " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |\n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): When set to ``False``, the properties of the tensors\n",
      " |              in the current module are preserved whereas setting it to ``True`` preserves\n",
      " |              properties of the Tensors in the state dict. The only\n",
      " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`\n",
      " |              for which the value from the module is preserved. Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * ``missing_keys`` is a list of str containing any keys that are expected\n",
      " |                  by this module but missing from the provided ``state_dict``.\n",
      " |              * ``unexpected_keys`` is a list of str containing the keys that are not\n",
      " |                  expected by this module but present in the provided ``state_dict``.\n",
      " |\n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |\n",
      " |  modules(self) -> collections.abc.Iterator['Module']\n",
      " |      Return an iterator over all modules in the network.\n",
      " |\n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |\n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |\n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |\n",
      " |  mtia(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the MTIA.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing the optimizer if the module will\n",
      " |      live on MTIA while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  named_buffers(\n",
      " |      self,\n",
      " |      prefix: str = '',\n",
      " |      recurse: bool = True,\n",
      " |      remove_duplicate: bool = True\n",
      " |  ) -> collections.abc.Iterator[tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |\n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |\n",
      " |  named_children(self) -> collections.abc.Iterator[tuple[str, 'Module']]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |\n",
      " |  named_modules(\n",
      " |      self,\n",
      " |      memo: Optional[set['Module']] = None,\n",
      " |      prefix: str = '',\n",
      " |      remove_duplicate: bool = True\n",
      " |  )\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |\n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |\n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |\n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |\n",
      " |  named_parameters(\n",
      " |      self,\n",
      " |      prefix: str = '',\n",
      " |      recurse: bool = True,\n",
      " |      remove_duplicate: bool = True\n",
      " |  ) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |\n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |\n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |\n",
      " |  parameters(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |\n",
      " |      This is typically passed to an optimizer.\n",
      " |\n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |\n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |\n",
      " |  register_backward_hook(\n",
      " |      self,\n",
      " |      hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]\n",
      " |  ) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |\n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_buffer(\n",
      " |      self,\n",
      " |      name: str,\n",
      " |      tensor: Optional[torch.Tensor],\n",
      " |      persistent: bool = True\n",
      " |  ) -> None\n",
      " |      Add a buffer to the module.\n",
      " |\n",
      " |      This is typically used to register a buffer that should not be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |\n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |\n",
      " |  register_forward_hook(\n",
      " |      self,\n",
      " |      hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]],\n",
      " |      *,\n",
      " |      prepend: bool = False,\n",
      " |      with_kwargs: bool = False,\n",
      " |      always_call: bool = False\n",
      " |  ) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |\n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |\n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |\n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |\n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |\n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_forward_pre_hook(\n",
      " |      self,\n",
      " |      hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]],\n",
      " |      *,\n",
      " |      prepend: bool = False,\n",
      " |      with_kwargs: bool = False\n",
      " |  ) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |\n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |\n",
      " |\n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |\n",
      " |          hook(module, args) -> None or modified input\n",
      " |\n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |\n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_full_backward_hook(\n",
      " |      self,\n",
      " |      hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],\n",
      " |      prepend: bool = False\n",
      " |  ) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |\n",
      " |      The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\n",
      " |\n",
      " |          1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\n",
      " |          2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\n",
      " |             with respect to module outputs.\n",
      " |          3. If none of the module outputs require gradients, then the hooks will not fire.\n",
      " |\n",
      " |      The hook should have the following signature::\n",
      " |\n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |\n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |\n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |\n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_full_backward_pre_hook(\n",
      " |      self,\n",
      " |      hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]],\n",
      " |      prepend: bool = False\n",
      " |  ) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |\n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |\n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |\n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |\n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |\n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |\n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |\n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |\n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |\n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |\n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |\n",
      " |  register_load_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\n",
      " |\n",
      " |      Arguments:\n",
      " |          hook (Callable): Callable hook that will be invoked before\n",
      " |              loading the state dict.\n",
      " |\n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |\n",
      " |  register_parameter(\n",
      " |      self,\n",
      " |      name: str,\n",
      " |      param: Optional[torch.nn.parameter.Parameter]\n",
      " |  ) -> None\n",
      " |      Add a parameter to the module.\n",
      " |\n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |\n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |\n",
      " |  register_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata) -> None\n",
      " |\n",
      " |      The registered hooks can modify the ``state_dict`` inplace.\n",
      " |\n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |\n",
      " |      It should have the following signature::\n",
      " |          hook(module, prefix, keep_vars) -> None\n",
      " |\n",
      " |      The registered hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |\n",
      " |  requires_grad_(self, requires_grad: bool = True) -> Self\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |\n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |\n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |\n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |\n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  set_extra_state(self, state: Any) -> None\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |\n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |\n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |\n",
      " |  set_submodule(self, target: str, module: 'Module', strict: bool = False) -> None\n",
      " |      Set the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |\n",
      " |      .. note::\n",
      " |          If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\n",
      " |          or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\n",
      " |          the method will only attempt to replace an existing submodule and throw an error if\n",
      " |          the submodule does not exist.\n",
      " |\n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |\n",
      " |      .. code-block:: text\n",
      " |\n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(3, 3, 3)\n",
      " |                  )\n",
      " |                  (linear): Linear(3, 3)\n",
      " |              )\n",
      " |          )\n",
      " |\n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |\n",
      " |      To override the ``Conv2d`` with a new submodule ``Linear``, you\n",
      " |      could call ``set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))``\n",
      " |      where ``strict`` could be ``True`` or ``False``\n",
      " |\n",
      " |      To add a new submodule ``Conv2d`` to the existing ``net_b`` module,\n",
      " |      you would call ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))``.\n",
      " |\n",
      " |      In the above if you set ``strict=True`` and call\n",
      " |      ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)``, an AttributeError\n",
      " |      will be raised because ``net_b`` does not have a submodule named ``conv``.\n",
      " |\n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |          module: The module to set the submodule to.\n",
      " |          strict: If ``False``, the method will replace an existing submodule\n",
      " |              or create a new submodule if the parent module exists. If ``True``,\n",
      " |              the method will only attempt to replace an existing submodule and throw an error\n",
      " |              if the submodule doesn't already exist.\n",
      " |\n",
      " |      Raises:\n",
      " |          ValueError: If the ``target`` string is empty or if ``module`` is not an instance of ``nn.Module``.\n",
      " |          AttributeError: If at any point along the path resulting from\n",
      " |              the ``target`` string the (sub)path resolves to a non-existent\n",
      " |              attribute name or an object that is not an instance of ``nn.Module``.\n",
      " |\n",
      " |  share_memory(self) -> Self\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |\n",
      " |  smart_apply(self, fn) from transformers.modeling_utils.PreTrainedModel.initialize_weights.<locals>\n",
      " |      # This function is equivalent to `torch.nn.Module.apply`, except that it dynamically adjust the function\n",
      " |      # to apply as we go down the graph\n",
      " |\n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |\n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |\n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |\n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |\n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |\n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |\n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |\n",
      " |      Example::\n",
      " |\n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |\n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |\n",
      " |      This can be called as\n",
      " |\n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |\n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |\n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |\n",
      " |      See below for examples.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |      Examples::\n",
      " |\n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |\n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |\n",
      " |  to_empty(\n",
      " |      self,\n",
      " |      *,\n",
      " |      device: Union[int, str, torch.device, NoneType],\n",
      " |      recurse: bool = True\n",
      " |  ) -> Self\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |\n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  train(self, mode: bool = True) -> Self\n",
      " |      Set the module in training mode.\n",
      " |\n",
      " |      This has an effect only on certain modules. See the documentation of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |\n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  type(self, dst_type: Union[torch.dtype, str]) -> Self\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  xpu(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |\n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |\n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |\n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |\n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |\n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |\n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |\n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |\n",
      " |  T_destination = ~T_destination\n",
      " |\n",
      " |  call_super_init = False\n",
      " |\n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14af544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['expected_answer', 'problem', 'generated_solution', 'Messages', 'N', 'text', '__index_level_0__'])\n",
      "Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer have? 17 You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "Place it between <start_working_out> and <end_working_out>.\n",
      "Then, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer have?<start_working_out>Okay, let's see. So the problem is about Jenifer who has 82 cents in pennies and nickels. But her younger brother thought all the nickels were dimes and counted the total as $1.47. We need to find out how many pennies Jenifer has. Hmm, let's break this down step by step.\n",
      "\n",
      "First, let me note down what we know. Let me denote the number of pennies as P and the number of nickels as N. Since pennies are worth 1 cent each and nickels 5 cents, the total value Jenifer has is 1*P + 5*N = 82 cents. That's the first equation.\n",
      "\n",
      "Now, her brother mistook nickels for dimes. Dimes are 10 cents each, right? So he counted the nickels as dimes. So the total he thought she had would be P*1 + N*10 = 147 cents (because $1.47 is 147 cents). So that's the second equation: P + 10N = 147.\n",
      "\n",
      "So now we have two equations:\n",
      "\n",
      "1) P + 5N = 82\n",
      "2) P + 10N = 147\n",
      "\n",
      "We can solve this system of equations to find P and N. Let me think, subtract the first equation from the second to eliminate P. Let's see:\n",
      "\n",
      "(P + 10N) - (P + 5N) = 147 - 82\n",
      "\n",
      "That simplifies to 5N = 65. Then N = 13. So there are 13 nickels.\n",
      "\n",
      "Now plug N back into the first equation to find P. So P + 5*13 = 82. 5*13 is 65, so P = 82 - 65 = 17. So Jenifer has 17 pennies. Let me check if this makes sense.\n",
      "\n",
      "If she has 17 pennies and 13 nickels, that's 17 + 65 = 82 cents. Correct. Then when her brother counts the nickels as dimes, that's 17 + 130 = 147 cents, which is $1.47. Yep, that matches the problem. So the answer should be 17 pennies.\n",
      "To solve the problem, we need to determine how many pennies Jenifer has given the following information:\n",
      "\n",
      "1. Jenifer has a total of 82 cents in pennies and nickels.\n",
      "2. Her younger brother, mistaking all nickels for dimes, counted the total as $1.47 (147 cents).\n",
      "\n",
      "Let's denote:\n",
      "- \\( P \\) as the number of pennies.\n",
      "- \\( N \\) as the number of nickels.\n",
      "\n",
      "We can set up the following system of equations based on the given information:\n",
      "\n",
      "1. The total value of the coins is 82 cents:\n",
      "   \\[\n",
      "   P + 5N = 82\n",
      "   \\]\n",
      "\n",
      "2. The mistaken count (nickels as dimes) totals 147 cents:\n",
      "   \\[\n",
      "   P + 10N = 147\n",
      "   \\]\n",
      "\n",
      "To find \\( P \\) and \\( N \\), we can solve this system of equations. First, subtract the first equation from the second to eliminate \\( P \\):\n",
      "\n",
      "\\[\n",
      "(P + 10N) - (P + 5N) = 147 - 82\n",
      "\\]\n",
      "\n",
      "This simplifies to:\n",
      "\n",
      "\\[\n",
      "5N = 65\n",
      "\\]\n",
      "\n",
      "Solving for \\( N \\):\n",
      "\n",
      "\\[\n",
      "N = \\frac{65}{5} = 13\n",
      "\\]\n",
      "\n",
      "Now that we have \\( N = 13 \\), we can substitute this value back into the first equation to find \\( P \\):\n",
      "\n",
      "\\[\n",
      "P + 5(13) = 82\n",
      "\\]\n",
      "\n",
      "Simplifying:\n",
      "\n",
      "\\[\n",
      "P + 65 = 82\n",
      "\\]\n",
      "\n",
      "Solving for \\( P \\):\n",
      "\n",
      "\\[\n",
      "P = 82 - 65 = 17\n",
      "\\]\n",
      "\n",
      "Thus, Jenifer has \\(\\boxed{17}\\) pennies.<end_working_out><SOLUTION>17</SOLUTION><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "sample=None\n",
    "for v in dataset:\n",
    "    print(v.keys())\n",
    "    print( v['problem'], v['expected_answer'], v['text'])\n",
    "    sample = v\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd9b1b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "Place it between <start_working_out> and <end_working_out>.\n",
      "Then, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have?<start_working_out>Okay, let's see. Jenifer has 82 cents in pennies and nickels. Her brother thought all the nickels were dimes and counted the total as $1.47. I need to find out how many pennies she has. Hmm, let me break this down.\n",
      "\n",
      "First, I need to set up some equations. Let's say the number of pennies is P and the number of nickels is N. The total value of the coins is 82 cents. So, the equation for the total value would be:\n",
      "\n",
      "1P + 5N = 82\n",
      "\n",
      "Because each penny is worth 1 cent and each nickel is worth 5 cents.\n",
      "\n",
      "Now, her brother mistook all the nickels for dimes. Dimes are worth 10 cents each. So, if he counted all the nickels as dimes, the total would be $1.47, which is 147 cents. So the equation for that would be:\n",
      "\n",
      "1P + 10N = 147\n",
      "\n",
      "Because the number of pennies is the same, but the nickels are now counted as dimes.\n",
      "\n",
      "Now I have two equations:\n",
      "\n",
      "1. P + 5N = 82\n",
      "2. P + 10N = 147\n",
      "\n",
      "I can solve these equations to find P and N. Let me subtract the first equation from the second to eliminate P.\n",
      "\n",
      "So, (P + 10N) - (P + 5N) = 147 - 82\n",
      "\n",
      "That simplifies to 5N = 65\n",
      "\n",
      "Then, dividing both sides by 5, N = 13\n",
      "\n",
      "So there are 13 nickels. Now plug that back into the first equation to find P.\n",
      "\n",
      "13 + 5N = 82\n",
      "\n",
      "Wait, no. Let me check. The first equation is P + 5N = 82. Since N is 13, then P + 5*13 = 82. So P + 65 = 82. Subtract 65 from both sides, P = 17\n",
      "\n",
      "So Jenifer has 17 pennies. Let me verify that. 17 pennies are 17 cents, and 13 nickels are 65 cents. Total is 82 cents, which matches the problem statement. Then, if the brother counts the nickels as dimes, 13 dimes would be 130 cents, plus the 17 pennies, which is 147 cents, or $1.47. That checks out. So the answer should be 17 pennies.\n",
      "To solve the problem, we start by setting up the equations based on the given information.\n",
      "\n",
      "1. Let \\( P \\) be the number of pennies and \\( N \\) be the number of nickels.\n",
      "2. The total value of the coins is 82 cents, so we have the equation:\n",
      "   \\[\n",
      "   P + 5N = 82\n",
      "   \\]\n",
      "3. Her brother mistook all the nickels for dimes and counted the total as $1.47, which is 147 cents. Therefore, we have the equation:\n",
      "   \\[\n",
      "   P + 10N = 147\n",
      "   \\]\n",
      "\n",
      "We now have a system of linear equations:\n",
      "\\[\n",
      "\\begin{cases}\n",
      "P + 5N = 82 \\\\\n",
      "P + 10N = 147\n",
      "\\end{cases}\n",
      "\\]\n",
      "\n",
      "To eliminate \\( P \\), we subtract the first equation from the second:\n",
      "\\[\n",
      "(P + 10N) - (P + 5N) = 147 - 82\n",
      "\\]\n",
      "\\[\n",
      "5N = 65\n",
      "\\]\n",
      "\\[\n",
      "N = 13\n",
      "\\]\n",
      "\n",
      "Now that we know \\( N = 13 \\), we substitute this value back into the first equation to find \\( P \\):\n",
      "\\[\n",
      "P + 5(13) = 82\n",
      "\\]\n",
      "\\[\n",
      "P + 65 = 82\n",
      "\\]\n",
      "\\[\n",
      "P = 17\n",
      "\\]\n",
      "\n",
      "Thus, Jenifer has \\(\\boxed{17}\\) pennies.<end_working_out><SOLUTION>17</SOLUTION><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#È¢ÑÂ§ÑÁêÜ‰∏Ä‰∏™Ê†∑Êú¨ ÁªôÊ®°ÂûãÊé®ÁêÜÔºå ÊúÄÈïøtokenÊï∞=1024\n",
    "text = tokenizer.apply_chat_template(\n",
    "    dataset[0][\"Messages\"][:2], # Message ÊòØ list ÂåÖÊã¨system_prompt, question, assistent_correct_answer. [:2]Âè™Âèñsystemm_prompt, question\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    temperature = 0,\n",
    "    max_new_tokens = 1024,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = False), # Âä†ÂÖ•ÊâìÂ≠óÊú∫ÊïàÊûú\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d7b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"<start_working_out>Okay, let's see. So the problem is about Jenifer who has 82 cents in pennies and nickels. But her younger brother thought all the nickels were dimes and counted the total as $1.47. We need to find out how many pennies Jenifer has. Hmm, let's break this down step by step.\\n\\nFirst, let me note down what we know. Let me denote the number of pennies as P and the number of nickels as N. Since pennies are worth 1 cent each and nickels 5 cents, the total value Jenifer has is 1*P + 5*N = 82 cents. That's the first equation.\\n\\nNow, her brother mistook nickels for dimes. Dimes are 10 cents each, right? So he counted the nickels as dimes. So the total he thought she had would be P*1 + N*10 = 147 cents (because $1.47 is 147 cents). So that's the second equation: P + 10N = 147.\\n\\nSo now we have two equations:\\n\\n1) P + 5N = 82\\n2) P + 10N = 147\\n\\nWe can solve this system of equations to find P and N. Let me think, subtract the first equation from the second to eliminate P. Let's see:\\n\\n(P + 10N) - (P + 5N) = 147 - 82\\n\\nThat simplifies to 5N = 65. Then N = 13. So there are 13 nickels.\\n\\nNow plug N back into the first equation to find P. So P + 5*13 = 82. 5*13 is 65, so P = 82 - 65 = 17. So Jenifer has 17 pennies. Let me check if this makes sense.\\n\\nIf she has 17 pennies and 13 nickels, that's 17 + 65 = 82 cents. Correct. Then when her brother counts the nickels as dimes, that's 17 + 130 = 147 cents, which is $1.47. Yep, that matches the problem. So the answer should be 17 pennies.\\nTo solve the problem, we need to determine how many pennies Jenifer has given the following information:\\n\\n1. Jenifer has a total of 82 cents in pennies and nickels.\\n2. Her younger brother, mistaking all nickels for dimes, counted the total as $1.47 (147 cents).\\n\\nLet's denote:\\n- \\\\( P \\\\) as the number of pennies.\\n- \\\\( N \\\\) as the number of nickels.\\n\\nWe can set up the following system of equations based on the given information:\\n\\n1. The total value of the coins is 82 cents:\\n   \\\\[\\n   P + 5N = 82\\n   \\\\]\\n\\n2. The mistaken count (nickels as dimes) totals 147 cents:\\n   \\\\[\\n   P + 10N = 147\\n   \\\\]\\n\\nTo find \\\\( P \\\\) and \\\\( N \\\\), we can solve this system of equations. First, subtract the first equation from the second to eliminate \\\\( P \\\\):\\n\\n\\\\[\\n(P + 10N) - (P + 5N) = 147 - 82\\n\\\\]\\n\\nThis simplifies to:\\n\\n\\\\[\\n5N = 65\\n\\\\]\\n\\nSolving for \\\\( N \\\\):\\n\\n\\\\[\\nN = \\\\frac{65}{5} = 13\\n\\\\]\\n\\nNow that we have \\\\( N = 13 \\\\), we can substitute this value back into the first equation to find \\\\( P \\\\):\\n\\n\\\\[\\nP + 5(13) = 82\\n\\\\]\\n\\nSimplifying:\\n\\n\\\\[\\nP + 65 = 82\\n\\\\]\\n\\nSolving for \\\\( P \\\\):\\n\\n\\\\[\\nP = 82 - 65 = 17\\n\\\\]\\n\\nThus, Jenifer has \\\\(\\\\boxed{17}\\\\) pennies.<end_working_out><SOLUTION>17</SOLUTION>\",\n",
       " 'role': 'assistant'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ê≠£Á°ÆÁ≠îÊ°à\n",
    "dataset[0][\"Messages\"][2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320c827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e11dfdc",
   "metadata": {},
   "source": [
    "# GRPO Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99979d65",
   "metadata": {},
   "source": [
    "### 1. Êï∞ÊçÆÈõÜÂáÜÂ§áÔºö‰ΩøÁî®GRPO Â∏¶rewardÁöÑÊï∞ÊçÆÈõÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8da70715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14116/14116 [00:00<00:00, 227113.97 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
       "    num_rows: 14116\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "rl_dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
    "rl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21def901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.',\n",
       " 'solution': '34',\n",
       " 'data_source': 'math_dapo',\n",
       " 'source_prompt': [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\\n\\nRemember to put your answer on its own line after \"Answer:\".',\n",
       "   'role': 'user'}],\n",
       " 'ability': 'MATH',\n",
       " 'reward_model': {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'},\n",
       " 'extra_info': {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba5c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14116/14116 [00:00<00:00, 22128.27 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': [{'content': 'You are given a problem.\\nThink about the problem step-by-step and provide your working out in a list to show the solution clearly and precisely.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>',\n",
       "   'role': 'system'},\n",
       "  {'content': 'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.',\n",
       "   'role': 'user'}],\n",
       " 'solution': '34',\n",
       " 'data_source': 'math_dapo',\n",
       " 'source_prompt': [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\\n\\nRemember to put your answer on its own line after \"Answer:\".',\n",
       "   'role': 'user'}],\n",
       " 'ability': 'MATH',\n",
       " 'reward_model': {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'},\n",
       " 'extra_info': {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'},\n",
       " 'answer': '34'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem step-by-step and provide your working out in a list to show the solution clearly and precisely.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "\n",
    "def extract_hash_answer(text):\n",
    "    # if \"####\" not in text: return None\n",
    "    # return text.split(\"####\")[1].strip()\n",
    "    return text\n",
    "# extract_hash_answer(dataset[0][\"solution\"])\n",
    "\n",
    "rl_dataset = rl_dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
    "})\n",
    "rl_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddea1ff",
   "metadata": {},
   "source": [
    "### 2. ËÆæËÆ°rewardÂáΩÊï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9ff8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Add optional EOS token matching\n",
    "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
    "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{reasoning_end}.*?\"\\\n",
    "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "##match_format\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Match if format is seen exactly!\n",
    "        if match_format.search(response) is not None: score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0][\"content\"]\n",
    "        # Count how many keywords are seen - we penalize if too many!\n",
    "        # If we see 1, then plus some points!\n",
    "\n",
    "        # No need to reward <start_working_out> since we always prepend it!\n",
    "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_format.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "        # Correct answer gets 5 points!\n",
    "        if guess == true_answer:\n",
    "            score += 5.0\n",
    "        # Match if spaces are seen, but less reward\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 3.5\n",
    "        else:\n",
    "            # We also reward it if the answer is close via ratios!\n",
    "            # Ie if the answer is within some range, reward it!\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
    "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
    "                else: score -= 2.5 # Penalize wrong answers\n",
    "            except:\n",
    "                score -= 4.5 # Penalize\n",
    "        scores.append(score)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6536ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.34']\n",
      "['123,456']\n",
      "['-0.234']\n",
      "['17']\n"
     ]
    }
   ],
   "source": [
    "global PRINTED_TIMES\n",
    "PRINTED_TIMES = 0\n",
    "global PRINT_EVERY_STEPS\n",
    "PRINT_EVERY_STEPS = 5\n",
    "\n",
    "match_numbers = re.compile(\n",
    "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")\n",
    "print(match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>  123,456  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>  -0.234  </SOLUTION>\"))\n",
    "print(match_numbers.findall(\"<SOLUTION>17</SOLUTION>\"))\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "    \n",
    "    extracted_responses = [\n",
    "        guess.group(1)\n",
    "        if (guess := match_numbers.search(r)) is not None else None \\\n",
    "        for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    # Print only every few steps\n",
    "    global PRINTED_TIMES\n",
    "    global PRINT_EVERY_STEPS\n",
    "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
    "        print(\n",
    "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
    "        )\n",
    "    PRINTED_TIMES += 1\n",
    "\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(-2.5)\n",
    "            continue\n",
    "        # Convert to numbers\n",
    "        try:\n",
    "            true_answer = float(true_answer.strip())\n",
    "            # Remove commas like in 123,456\n",
    "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
    "            scores.append(3.5 if guess == true_answer else -1.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b448e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14116/14116 [00:03<00:00, 4178.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized[0]:  {'prompt': [{'content': 'You are given a problem.\\nThink about the problem step-by-step and provide your working out in a list to show the solution clearly and precisely.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>', 'role': 'system'}, {'content': 'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.', 'role': 'user'}], 'solution': '34', 'data_source': 'math_dapo', 'source_prompt': [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\\n\\nRemember to put your answer on its own line after \"Answer:\".', 'role': 'user'}], 'ability': 'MATH', 'reward_model': {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'}, 'extra_info': {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'}, 'answer': '34', 'tokens': [2610, 525, 2661, 264, 3491, 624, 38687, 911, 279, 3491, 3019, 14319, 29208, 323, 3410, 697, 3238, 700, 304, 264, 1140, 311, 1473, 279, 6291, 9355, 323, 23638, 624, 17371, 432, 1948, 366, 2468, 81101, 6068, 29, 323, 366, 408, 81101, 6068, 29816, 12209, 11, 3410, 697, 6291, 1948, 366, 50, 45977, 1472, 50, 45977, 29, 151643, 641, 21495, 400, 25411, 54876, 57960, 15940, 1124, 4044, 362, 284, 1124, 37018, 90, 19, 15170, 20, 31716, 323, 57960, 4044, 362, 366, 220, 24, 15, 24884, 43298, 12947, 6771, 400, 35, 3, 387, 264, 1459, 4889, 21495, 400, 25411, 3, 1741, 429, 57960, 4044, 47718, 284, 1124, 4044, 39196, 3, 323, 57960, 4044, 425, 5626, 284, 220, 24, 15, 24884, 43298, 12947, 82610, 429, 400, 1808, 284, 220, 16, 3, 323, 429, 57960, 37018, 90, 9548, 15170, 6484, 92, 284, 1124, 37018, 90, 18, 15170, 17, 92, 12947, 1416, 400, 1867, 488, 10584, 3, 646, 387, 13302, 304, 279, 1352, 57960, 37018, 90, 64, 59, 26888, 90, 65, 3417, 90, 66, 31716, 1380, 400, 64, 11, 293, 11, 272, 3, 525, 92759, 12040, 10250, 25780, 11, 1477, 400, 64, 488, 293, 488, 272, 12947, 27, 2468, 81101, 6068, 29]}\n",
      "You are given a problem.\n",
      "Think about the problem step-by-step and provide your working out in a list to show the solution clearly and precisely.\n",
      "Place it between <start_working_out> and <end_working_out>.\n",
      "Then, provide your solution between <SOLUTION></SOLUTION><|endoftext|>In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.<start_working_out>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14116/14116 [-1:59:50<00:00, -1404.91 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length =  214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Ë∞ÉÁî®‰∏äÈù¢ÁöÑsystem_promptÁöÑÊèêÁ§∫ËØçÂêéÔºåÂÜçtokenizer ËæìÂá∫tokens ids ‰ª•Âèäbatching\n",
    "tokenized = rl_dataset.map(\n",
    "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
    "    batched = True,\n",
    ")\n",
    "print(\"tokenized[0]: \", tokenized[0])\n",
    "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
    "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
    "\n",
    "import numpy as np\n",
    "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
    "print(\"Max Length = \", maximum_length)\n",
    "\n",
    "# Filter only samples smaller than 90% max length\n",
    "rl_dataset = rl_dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
    "del tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e60d064f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info', 'answer'],\n",
       "    num_rows: 12709\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34d7d8",
   "metadata": {},
   "source": [
    "### 3. GRPO RLHF training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55706555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "from vllm import SamplingParams\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
    "max_completion_length = max_seq_length - max_prompt_length\n",
    "\n",
    "vllm_sampling_params = SamplingParams(\n",
    "    min_p = 0.1,\n",
    "    top_p = 1.0,\n",
    "    top_k = -1,\n",
    "    seed = 3407,\n",
    "    stop = [tokenizer.eos_token],\n",
    "    include_stop_str_in_output = True,\n",
    ")\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    vllm_sampling_params = vllm_sampling_params,\n",
    "    temperature = 1.0,\n",
    "    learning_rate = 5e-6,\n",
    "    weight_decay = 0.05,\n",
    "    warmup_ratio = 0.1,\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    optim = \"adamw_8bit\",\n",
    "    logging_steps = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    bf16 = is_bfloat16_supported(),\n",
    "    fp16 = not is_bfloat16_supported(),\n",
    "    gradient_accumulation_steps = 4, # Increase to 4 for smoother training\n",
    "    num_generations = 4, # Decrease if out of memory\n",
    "    max_prompt_length = max_prompt_length,\n",
    "    max_completion_length = max_completion_length,\n",
    "    num_train_epochs = 1, # Set to 1 for a full training run\n",
    "    # max_steps = 100,\n",
    "    # save_steps = 100,\n",
    "    max_steps = 30,\n",
    "    save_steps = 30,\n",
    "    report_to = \"none\", # Can use Weights & Biases\n",
    "    output_dir = \"outputs\",\n",
    "\n",
    "    generation_kwargs={\n",
    "        # \"max_new_tokens\": 128,\n",
    "        \"top_p\": 0.9,\n",
    "    },\n",
    "    # For optional training + evaluation\n",
    "    # fp16_full_eval = True,\n",
    "    # per_device_eval_batch_size = 4,\n",
    "    # eval_accumulation_steps = 1,\n",
    "    # eval_strategy = \"steps\",\n",
    "    # eval_steps = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98a3d68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 12,709 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Question:\n",
      "Compute the number of positive integers that divide at least two of the integers in the set $\\{1^1,2^2,3^3,4^4,5^5,6^6,7^7,8^8,9^9,10^{10}\\}$. \n",
      "Answer:\n",
      "22 \n",
      "Response:\n",
      "Okay, so I need to find the number of positive integers that divide at least two of the numbers in the set {1^1, 2^2, 3^3, 4^4, 5^5, 6^6, 7^7, 8^8, 9^9, 10^10}. Hmm, let's start by understanding the problem. I need to find all the divisors of at least two different numbers in this set. So, maybe I can think of the divisors of each number and then find the common divisors between at least two of them. But since the exponents are high, some of these numbers are very large, and their divisors would be quite big. But the question is about the number of such divisors, not the divisors themselves. So maybe I can use the concept of the greatest common divisor (GCD) and the fact that a number is a common divisor of two numbers if and only if it divides their GCD. But since the question is about at least two, perhaps I can use the principle of inclusion-exclusion. Let me think.\n",
      "\n",
      "First, let's list the numbers in the set. They are: 1, 4, 27, 256, 3125, 46656, 823543, 16777216, 387420489, 10000000000. Wait, but actually, the exponents are 1,2,3,4,5,6,7,8,9,10, so the numbers are 1^1, 2^2, 3^3, etc. So the set is {1, 4, 27, 256, 3125, 46656, 823543, 16777216, 387420489, 10000000000}. But when I look at the exponents, the number of divisors of a number n = p1^a1 * p2^a2 * ... * pk^ak is (a1+1)(a2+1)...(ak+1). So for each number, the number of divisors is the product of one more than each of the exponents in its prime factorization. But since the exponents are high, the number of divisors is also high. But the question is about the number of divisors that are common to at least two of them. So maybe I can find the GCD of every pair of numbers and then find the number of divisors of each GCD. But that would be a lot of pairs. There are 10 numbers, so 10 choose 2 = 45 pairs. But some of these GCDs might be the same, so maybe I can find the number of unique GCDs. But the question is about the number of divisors of at least one of them, not the GCDs. Wait, but if a number is a common divisor of two numbers, it's a divisor of their GCD. So the number of divisors of the GCDs of all pairs. But the question is about the number of divisors that are in at least one of the GCDs. So that's the sum of the number of divisors of each GCD, but since some GCDs might have the same divisors, I need to use the principle of inclusion-exclusion. But that might be complicated. Maybe there's a smarter way. Let me think.\n",
      "\n",
      "Alternatively, maybe I can find the number of divisors of each number and then use the principle of inclusion-exclusion. The number of divisors of a number n is the product of (e1+1)(e2+1)...(ek+1) where n = p1^e1 * p2^e2 * ... * pk^ek. So for each number, I can find its prime factorization and then its number of divisors. Then, the total number of divisors across all numbers is the sum of the divisors of each number. But some divisors are counted multiple times if they divide more than one number. So the number of divisors that divide at least two numbers is the total number of divisors minus the number of divisors that divide only one number. So maybe I can compute the total number of divisors and subtract the number of divisors that divide only one number. Let me check if that works.\n",
      "\n",
      "Let me compute the number of divisors for each number:\n",
      "\n",
      "1. 1^1 = 1: divisors are {1}, so 1 divisor.\n",
      "2. 2^2 = 4: divisors are {1, 2, 4}, so 3 divisors.\n",
      "3. 3^3 = 27: divisors are {1, 3, 9, 27}, so 4 divisors.\n",
      "4. 4^4 = 256: divisors are {1, 2, 4, 8, 16, 32, 64, 128, 256}, so 9 divisors.\n",
      "5. 5^5 = 3125: divisors are {1, 5, 25, 125, 625, 3125}, so 6 divisors.\n",
      "6. 6^6 = 46656: prime factors are 2^6 * 3^6. Divisors are (6+1)(6+1)=49.\n",
      "7. 7^7 = 823543: prime factors are 7^7. Divisors are (7+1)=8.\n",
      "8. 8^8 = 16777216: prime factors are 2^8. Divisors are (8+1)=9.\n",
      "9. 9^9 = 387420489: prime factors are 3^9. Divisors are (9+1)=10.\n",
      "10. 10^10 = 10000000000: prime factors are 2^10 * 5^10. Divisors are (10+1)(10+1)=121.\n",
      "\n",
      "Now, summing these up: 1 + 3 + 4 + 9 + 6 + 49 + 8 + 9 + 10 + 121 = 230 total divisors.\n",
      "\n",
      "Now, the divisors that divide only one number are those that divide exactly one number in the list. So the divisors that divide two numbers must have appeared in two numbers. Therefore, the divisors that divide at least two numbers are those that appear in multiple pairs. So perhaps I can compute the total divisors across all pairs and subtract the ones that appear once.\n",
      "\n",
      "Wait, but actually, each divisor that divides multiple numbers would be counted multiple times in the pairs. So maybe the correct approach is to compute the total number of pairs where each divisor appears and then use inclusion-exclusion. Alternatively, perhaps there's a smarter way by considering the prime factors.\n",
      "\n",
      "Let me think differently. Maybe instead of computing each divisor count, I can consider the prime factors and use the principle that a divisor must divide both numbers if it divides their greatest common divisor (GCD). So the problem reduces to finding all pairs (i,j) such that gcd(a_i, a_j) > 1. Then the number of common divisors is equal to the number of divisors of gcd(a_i, a_j), which is œÜ(gcd(a_i, a_j)), where œÜ is Euler's totient function. Wait, no. Actually, the number of positive divisors of gcd(a_i, a_j) is œÑ(gcd(a_i, a_j)), where œÑ(n) is the divisor function. So each pair with gcd greater than 1 contributes œÑ(gcd(a_i, a_j)) divisors. Therefore, the total number of divisors that divide at least two numbers is the sum of œÑ(gcd(a_i, a_j)) over all pairs (i,j).\n",
      "\n",
      "So perhaps I need to compute œÑ(gcd(a_i, a_j)) for each pair and sum them up. But since some gcd values will repeat across different pairs, I can compute the number of times each gcd value appears in the set. For each gcd, the number of pairs (i,j) with gcd(a_i, a_j \n",
      "Extracted:\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 3:31:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>reward</th>\n",
       "      <th>reward_std</th>\n",
       "      <th>completions / mean_length</th>\n",
       "      <th>completions / min_length</th>\n",
       "      <th>completions / max_length</th>\n",
       "      <th>completions / clipped_ratio</th>\n",
       "      <th>completions / mean_terminated_length</th>\n",
       "      <th>completions / min_terminated_length</th>\n",
       "      <th>completions / max_terminated_length</th>\n",
       "      <th>sampling / sampling_logp_difference / mean</th>\n",
       "      <th>sampling / sampling_logp_difference / max</th>\n",
       "      <th>sampling / importance_sampling_ratio / min</th>\n",
       "      <th>sampling / importance_sampling_ratio / mean</th>\n",
       "      <th>sampling / importance_sampling_ratio / max</th>\n",
       "      <th>kl</th>\n",
       "      <th>rewards / match_format_exactly / mean</th>\n",
       "      <th>rewards / match_format_exactly / std</th>\n",
       "      <th>rewards / match_format_approximately / mean</th>\n",
       "      <th>rewards / match_format_approximately / std</th>\n",
       "      <th>rewards / check_answer / mean</th>\n",
       "      <th>rewards / check_answer / std</th>\n",
       "      <th>rewards / check_numbers / mean</th>\n",
       "      <th>rewards / check_numbers / std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.098384</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>4.618802</td>\n",
       "      <td>1649.500000</td>\n",
       "      <td>1443.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1466.000000</td>\n",
       "      <td>1443.000000</td>\n",
       "      <td>1489.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.118224</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.598076</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.096776</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.121139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.130537</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.096383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-3.500000</td>\n",
       "      <td>4.618802</td>\n",
       "      <td>1652.500000</td>\n",
       "      <td>1342.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1472.000000</td>\n",
       "      <td>1342.000000</td>\n",
       "      <td>1602.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.117091</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.598076</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.288675</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.081524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.076354</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.096229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>11.835680</td>\n",
       "      <td>1656.000000</td>\n",
       "      <td>1238.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1479.000000</td>\n",
       "      <td>1238.000000</td>\n",
       "      <td>1720.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.107954</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.598076</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.041452</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.464102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.065092</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1767.000000</td>\n",
       "      <td>1569.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1569.000000</td>\n",
       "      <td>1569.000000</td>\n",
       "      <td>1569.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.089958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.089212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>1571.250000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1748.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.093821</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.103771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.145350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.064273</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.117172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.131206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>9.724325</td>\n",
       "      <td>1461.250000</td>\n",
       "      <td>791.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1089.500000</td>\n",
       "      <td>791.000000</td>\n",
       "      <td>1388.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.129264</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.125000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.464102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>10.070584</td>\n",
       "      <td>1531.500000</td>\n",
       "      <td>1210.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1230.000000</td>\n",
       "      <td>1210.000000</td>\n",
       "      <td>1250.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.105766</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.598076</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>3.403430</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.872281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1520.000000</td>\n",
       "      <td>695.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1207.000000</td>\n",
       "      <td>695.000000</td>\n",
       "      <td>1719.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.122119</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>-2.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.097442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>11.835680</td>\n",
       "      <td>1664.750000</td>\n",
       "      <td>1268.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1496.500000</td>\n",
       "      <td>1268.000000</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.134729</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.732051</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>2.598076</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.041452</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3.464102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1587.750000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.077252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-5.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1674.500000</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.082821</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>-1.875000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>-2.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1797.000000</td>\n",
       "      <td>1689.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1689.000000</td>\n",
       "      <td>1689.000000</td>\n",
       "      <td>1689.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.065148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>-7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>0.097159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************Question:\n",
      "The sum of $\\lfloor x \\rfloor$ for all real numbers $x$ satisfying the equation $16 + 15x + 15x^2 = \\lfloor x \\rfloor^3$ is: \n",
      "Answer:\n",
      "33 \n",
      "Response:\n",
      "Okay, so I need to find the sum of the floor of x, denoted as ‚åäx‚åã, for all real numbers x that satisfy the equation 16 + 15x + 15x¬≤ = ‚åäx‚åã¬≥. Hmm, let's break this down.\n",
      "\n",
      "First, I need to find all real x where the left side, 16 + 15x + 15x¬≤, equals the cube of the floor of x. The floor function, ‚åäx‚åã, gives the greatest integer less than or equal to x. So, for each possible integer k, I can set ‚åäx‚åã = k and then solve the equation 16 + 15x + 15x¬≤ = k¬≥. But then, the solution x must also satisfy k ‚â§ x < k+1. So, for each k, I need to find the roots of the equation 15x¬≤ + 15x + 16 - k¬≥ = 0, and then check if the solution x is in the interval [k, k+1).\n",
      "\n",
      "Let me start by trying some integer values for k. Since the equation is a quadratic in x, the solutions will be x = [ -15 ¬± sqrt(15¬≤ - 4*15*(16 - k¬≥)) ] / (2*15) = [ -15 ¬± sqrt(225 - 60(16 - k¬≥)) ] / 30. But that might be a bit messy. Maybe I can look for integer k where 15x¬≤ + 15x + 16 - k¬≥ = 0 has a real solution. The discriminant D = 225 - 60(16 - k¬≥) must be non-negative. So, 225 - 60(16 - k¬≥) ‚â• 0 => 225 - 960 + 60k¬≥ ‚â• 0 => 60k¬≥ - 735 ‚â• 0 => k¬≥ ‚â• 12.25. So k must be at least 2 because 2¬≥=8, 3¬≥=27, so k=2 is the smallest integer where the equation could have a real solution.\n",
      "\n",
      "Let's start with k=2. Plugging into the equation: 15x¬≤ + 15x + 16 - 8 = 0 => 15x¬≤ + 15x + 8 = 0. The discriminant is 225 - 4*15*8 = 225 - 480 = -255, which is negative. So no real solutions for k=2. Next, k=3. Equation: 15x¬≤ + 15x + 16 - 27 = 0 => 15x¬≤ + 15x -11 = 0. Discriminant: 225 - 4*15*(-11) = 225 +660 = 885, which is positive. So there are two real solutions. Let's find them. x = [ -15 ¬± ‚àö885 ] / 30. The floor of x would be 0 if the solution is between 0 and 1, but let's check. The positive root is ( -15 + ‚àö885 ) / 30. Since ‚àö885 is between 29 and 30, the value is between ( -15 + 29 )/30 = 14/30 = 0.666... and ( -15 +30 )/30 = 1.5. So the solution is between 0.666 and 1.5, so the floor is 0. But the equation is 15x¬≤ +15x -11=0, and the solution is ( -15 + ‚àö885 )/30. Let's compute it more precisely. ‚àö885 ‚âà 29.75, so x ‚âà ( -15 + 29.75 )/30 ‚âà 14.75/30 ‚âà 0.4917. So floor is 0. So k=3 gives a solution with floor 0. But wait, we need to check if it's between 3 and 4. No, because the solution is less than 1. So maybe k=3 doesn't work? Wait, no, k=3 would require x to be between 3 and 4. But our solution is less than 1. So no solution here. Hmm, maybe I made a mistake. Let me check again. For k=3, the equation is 15x¬≤ +15x -11=0. The discriminant is 885, so two real roots. The smaller root is ( -15 - ‚àö885 )/30 ‚âà (-15 - 29.75)/30 ‚âà -44.75/30 ‚âà -1.4917. So one root is negative, and the other is positive. The positive root is approximately 0.4917. So floor is 0. But for k=3, we need x to be between 3 and 4. Since the solution is less than 1, it doesn't satisfy the condition. So no solution here. Therefore, k=3 doesn't work.\n",
      "\n",
      "Next, k=4. Equation: 15x¬≤ +15x +16 - 64 = 0 => 15x¬≤ +15x -48 = 0. Discriminant: 225 - 4*15*(-48) = 225 + 2880 = 3105. So two real roots. Let's compute them: x = [ -15 ¬± ‚àö3105 ] / 30. ‚àö3105 ‚âà 55.72. So x ‚âà (-15 + 55.72)/30 ‚âà 40.72/30 ‚âà 1.357 and (-15 - 55.72)/30 ‚âà -70.72/30 ‚âà -2.357. So the solutions are approximately 1.357 and -2.357. We need x between 4 and 5. Since neither solution is between 4 and 5, no solutions here either. So k=4 doesn't work.\n",
      "\n",
      "Next, k=5. Equation: 15x¬≤ +15x +16 - 125 = 0 => 15x¬≤ +15x -109 = 0. Discriminant: 225 - 4*15*(-109) = 225 + 6540 = 6765. Two real roots: x = [ -15 ¬± ‚àö6765 ] / 30. ‚àö6765 ‚âà 82.26. So x ‚âà (-15 + 82.26)/30 ‚âà 67.26/30 ‚âà 2.242 and (-15 - 82.26)/30 ‚âà -97.26/30 ‚âà -3.242. Need x between 5 and 6. Neither solution is between 5 and 6. So no solutions here either.\n",
      "\n",
      "Wait, maybe k needs to be smaller than 5? Earlier I thought k‚â•2 because k¬≥‚â•12.25. Let me check k=1. Equation: 15x¬≤ +15x +16 -1 = 0 => 15x¬≤ +15x +15 = 0 => x¬≤ + x +1 = 0. Discriminant is 1 - 4 = -3. No real solutions here. So k cannot be 1 either.\n",
      "\n",
      "Therefore, the only possible integer k where the equation has real solutions is k=2, but that didn't work. So there are no real solutions. But wait, maybe I made a mistake in the earlier steps. Let me re-examine. The original equation is 16 +15x +15x \n",
      "Extracted:\n",
      "None\n",
      "********************Question:\n",
      "Letters $A, B, C,$ and $D$ represent four different digits from 0, 1, 2, 3, ..., 9. If $\\frac{A+B}{C+D}$ is an integer that is as large as possible, what is the value of $A+B$? \n",
      "Answer:\n",
      "17 \n",
      "Response:\n",
      "Okay, so I need to find four different digits A, B, C, and D from 0 to 9 such that the fraction (A + B) / (C + D) is an integer and as large as possible. I need to find the value of A + B. Let me think.\n",
      "\n",
      "First, to maximize the value of the fraction (A + B)/(C + D), I need the numerator (A + B) to be as large as possible and the denominator (C + D) to be as small as possible, right? Because a larger top and a smaller bottom will make the fraction bigger. But there's a catch: A, B, C, and D are all different digits. So when I pick the largest possible A and B, I have to make sure C and D are the smallest possible different digits.\n",
      "\n",
      "Let's start by trying the largest possible sum for A and B. The maximum sum of two different digits is 9 + 8 = 17. But then, the smallest possible sum for C and D would be 0 + 1 = 1, which would make the fraction 17/1 = 17, which is an integer. But wait, is 17/1 the maximum possible? Let me check if there's a larger fraction.\n",
      "\n",
      "If I take A=9, B=8, then the sum is 17. The next largest sum for C and D would be 0+1=1, so 17/1=17. If I try to make the denominator even smaller, but the digits have to be different. The next smallest would be 0+2=2, so 17/2=8.5, which is less than 17. So 17/1 is better. What if I use a different pair for A and B? For example, 9+7=16, which is less than 17. So 17/1 is still the maximum. \n",
      "\n",
      "But wait, can I have a larger sum for A and B? The next would be 9+6=15, which is less. So 17 is the maximum. So the answer would be 17. But let me make sure I'm not missing any cases. For example, if I use 8+7=15, which is less. 9+5=14, also less. 9+4=13, even less. So 17 is the maximum. And the smallest possible sum for C and D is 0+1=1, so 17/1=17. So the answer is 17. But wait, the problem says the digits are different, and 0,1,2,3,4,5,6,7,8,9. If I use 0+1, that's valid. So 17/1 is an integer, and that's the maximum. So the answer is 17. But let me check if there's a way to have a larger sum in the numerator. If I use 9+8, that's 17. If I use 9+7, 15, which is less. 9+6, 15, also less. 9+5, 14, even less. So 17 is the maximum. So the answer is 17. I think that's it. So the value of A + B is 17. So the answer is 17. But let me make sure I didn't make a mistake. If I use 9+8, the fraction is 17/1, which is 17. If I use 9+7, 15/2=7.5, which is less. 9+6, 15/3=5, which is less. 9+5, 14/4=3.5, which is less. 9+4, 13/5=2.6, which is less. 9+3, 12/6=2, which is less. 9+2, 11/7‚âà1.57, which is less. 9+1, 10/8=1.25, which is less. 9+0, 9/9=1, which is less. So the maximum is when A and B are 9 and 8, giving 17. So the answer is 17. So the solution is 17.\n",
      "To maximize the value of the fraction \\(\\frac{A+B}{C+D}\\) where \\(A, B, C,\\) and \\(D\\) are distinct digits from 0 to 9, we need to maximize the numerator \\(A+B\\) and minimize the denominator \\(C+D\\).\n",
      "\n",
      "1. **Maximize the Numerator \\(A+B\\):**\n",
      "   - The largest possible sum of two distinct digits is \\(9 + 8 = 17\\).\n",
      "\n",
      "2. **Minimize the Denominator \\(C+D\\):**\n",
      "   - The smallest possible sum of two distinct digits is \\(0 + 1 = 1\\).\n",
      "\n",
      "3. **Check the Fraction:**\n",
      "   - If \\(A = 9\\), \\(B = 8\\), then \\(A + B = 17\\).\n",
      "   - If \\(C = 0\\) and \\(D = 1\\), then \\(C + D = 1\\).\n",
      "   - The fraction \\(\\frac{A+B}{C+D} = \\frac{17}{1} = 17\\) is an integer.\n",
      "\n",
      "4. **Verify Other Possibilities:**\n",
      "   - Any other combination of \\(A\\) and \\(B\\) will result in a smaller numerator.\n",
      "   - Any other combination of \\(C\\) and \\(D\\) will result in a larger denominator unless \\(C\\) and \\(D\\) are chosen such that \\(C + D\\) is minimized while ensuring distinct digits.\n",
      "   - For example:\n",
      "     - \\(A + B = 16\\) (e.g., \\(A = 9\\), \\(B = 7\\)): \\(\\frac{16}{2} = 8\\) (smaller than 17).\n",
      "     - \\(A + B = 15\\) (e.g., \\(A = 9\\), \\(B = 6\\)): \\(\\frac{15}{3} = 5\\) (smaller than 17).\n",
      "     - \\(A + B = 14\\) (e.g., \\(A = 9\\), \\(B = 5\\)): \\(\\frac{14}{5} = 2.8\\) (smaller than 17).\n",
      "     - \\(A + B = 13\\) (e.g., \\(A = 9\\), \\(B = 4\\)): \\(\\frac{13}{6} \\approx 2.17\\) (smaller than 17).\n",
      "     - \\(A + B = 12\\) (e.g., \\(A = 9\\), \\(B = 3\\)): \\(\\frac{12}{8} = 1.5\\) (smaller than 17).\n",
      "     - \\(A + B = 11\\) (e.g., \\(A = 9\\), \\(B = 2\\)): \\(\\frac{11}{9} \\approx 1.22\\) (smaller than 17).\n",
      "     - \\(A + B = 10\\) (e.g., \\(A = 9\\), \\(B = 1\\)): \\(\\frac{10}{8} = 1.25\\) (smaller than 17).\n",
      "     - \\(A + B = 9\\) (e.g., \\(A = 9\\), \\(B = 0\\)): \\(\\frac{9}{9} = 1\\) (smaller than 17).\n",
      "\n",
      "Thus, the maximum value of \\(A+B\\) that satisfies the conditions is \\(\\boxed{17}\\).<end_working_out><SOLUTION>17</SOLUTION> \n",
      "Extracted:\n",
      "17\n",
      "********************Question:\n",
      "Let $f(x)=\\int_0^{x} e^{t} (\\cos t+\\sin t)\\, dt$, $g(x)=\\int_0^{x} e^{t} (\\cos t-\\sin t)\\, dt$. \n",
      "\n",
      "For a real number $a$, find:\n",
      "\\[ \\sum_{n=1}^{\\infty} \\frac{e^{2a}}{\\{f^{(n)}(a)\\}^2+\\{g^{(n)}(a)\\}^2}. \\] \n",
      "Answer:\n",
      "1 \n",
      "Response:\n",
      "Okay, so I have this problem here. It's about two functions, f and g, which are defined as integrals of some expressions involving e^t, sin t, and cos t. The problem asks to find the sum from n=1 to infinity of 1 divided by the sum of the squares of the nth derivatives of f and g evaluated at a, all divided by e raised to 2a. Hmm, that's a bit complex. Let me try to break this down step by step.\n",
      "\n",
      "First, I need to find the derivatives of f and g. Let's start with f(x) = ‚à´ from 0 to x of e^t (cos t + sin t) dt. The derivative of f with respect to x would be the integrand evaluated at x, right? So f'(x) = e^x (cos x + sin x). Then, the second derivative f''(x) would be the derivative of f'(x). Let's compute that. The derivative of e^x (cos x + sin x) is e^x (cos x + sin x) + e^x (-sin x + cos x) = 2e^x cos x. So f''(x) = 2e^x cos x. Wait, that's interesting. Let me check the next derivative. f'''(x) would be 2e^x cos x + 2e^x (-sin x) = 2e^x (cos x - sin x). Then f''''(x) = 2e^x (cos x - sin x) + 2e^x (-sin x - cos x) = 2e^x (-2 sin x) = -4e^x sin x. So the pattern here is that the derivatives of f are oscillating between e^x times some combination of sin and cos. \n",
      "\n",
      "Now, for g(x) = ‚à´ from 0 to x of e^t (cos t - sin t) dt. The first derivative g'(x) is e^x (cos x - sin x). The second derivative g''(x) is e^x (cos x - sin x) + e^x (-sin x - cos x) = 0. So the second derivative of g is zero. That's a big deal! So g''(x) = 0, which means that g(n) = 0 for n > 1. That simplifies things. So g''(a) = 0, g'''(a) = 0, and so on. So the sum in the problem will have all terms from n=1 to infinity where the term for n>1 is 0. So the only non-zero term is n=1. \n",
      "\n",
      "So the sum becomes 1 / {f'(a)^2 + 0^2} = 1 / f'(a)^2. But f'(a) = e^a (cos a + sin a). So the sum is 1 / (e^a (cos a + sin a))^2. Now, the problem is to find the sum from n=1 to infinity of 1 / {f(n)(a)^2 + g(n)(a)^2}. But since g(n) = 0 for n>1, the only term in the sum is n=1, so the sum is 1 / f'(a)^2. So the answer is 1 / (e^a (cos a + sin a))^2. But let me check if I can simplify this further. The denominator is e^(2a) (cos^2 a + 2 sin a cos a + sin^2 a) = e^(2a) (1 + 2 sin a cos a) = e^(2a) (1 + sin 2a). So the sum is 1 / (e^(2a) (1 + sin 2a)) = e^(-2a) / (1 + sin 2a). But the problem has 1 divided by the sum of the squares, so maybe there's another way. Wait, let me check again. The original problem is sum from n=1 to infinity of 1 / {f(n)(a)^2 + g(n)(a)^2}. Since g(n) = 0 for n>1, the sum is just 1 / f'(a)^2. So the answer should be 1 / (e^a (cos a + sin a))^2. Let me verify that again. If I compute f'(a), it's e^a (cos a + sin a), so squaring that gives e^(2a) (cos^2 a + 2 sin a cos a + sin^2 a) = e^(2a) (1 + sin 2a). So the reciprocal is e^(-2a) / (1 + sin 2a). So the sum is e^(-2a) / (1 + sin 2a). That seems right. Let me check if there's another approach. Maybe using integration by parts or something. Let me think again. \n",
      "\n",
      "Alternatively, perhaps there's a pattern in the derivatives of f and g. Let me compute the second derivatives again. For f(x), f''(x) = 2e^x cos x. Then f'''(x) = 2e^x cos x - 2e^x sin x. Then f''''(x) = 2e^x cos x - 4e^x sin x + 2e^x cos x = 4e^x cos x - 4e^x sin x. Hmm, maybe there's a recurrence relation here. Similarly for g(x), g'(x) = e^x (cos x - sin x), g''(x) = 0. So higher derivatives are zero. So only f'(x) contributes to the sum. Therefore, the sum is just 1 / f'(a)^2. So the answer should be e^(-2a) / (1 + sin 2a). Let me confirm by computing f'(a): e^a (cos a + sin a), squared gives e^(2a) (cos^2 a + 2 sin a cos a + sin^2 a) = e^(2a) (1 + sin 2a). So the reciprocal is e^(-2a) / (1 + sin 2a). Therefore, the sum is that value. So that seems correct.\n",
      "To solve the problem, we start by defining the functions \\( f(x) \\) and \\( g(x) \\):\n",
      "\n",
      "\\[ f(x) = \\int_0^x e^t (\\cos t + \\sin t) \\, dt \\]\n",
      "\\[ g(x) = \\int_0^x e^t (\\cos t - \\sin t) \\, dt \\]\n",
      "\n",
      "Next, we compute the derivatives of \\( f(x) \\) and \\( g(x) \\):\n",
      "\n",
      "\\[ f'(x) = e^x (\\cos x + \\sin x) \\]\n",
      "\\[ g'(x) = e^x (\\cos x - \\sin x) \\]\n",
      "\n",
      "We then compute the second derivatives:\n",
      "\n",
      "\\[ f''(x) = \\frac{d}{dx} \\left[ e^x (\\cos x + \\sin x) \\right] = e^x (\\cos x + \\sin x) + e^x (-\\sin x + \\cos x) = 2e^x \\cos x \\]\n",
      "\n",
      "\\[ g''(x) = \\frac{d}{dx} \\left[ e^x (\\cos x - \\sin x) \\right] = e^x (\\cos x - \\sin x) + e^x (-\\sin x - \\cos x) = -2e^x \\sin x \\]\n",
      "\n",
      "Notice that \\( g''(x) = -2e^x \\sin x \\), but higher derivatives of \\( g(x) \\) will involve higher-order trigonometric functions which will eventually simplify to zero due to the oscillatory nature and the exponential decay.\n",
      "\n",
      "Now, we consider the sum:\n",
      "\n",
      "\\[ \\sum_{n=1}^{\\infty} \\frac{e^{2a}}{\\{f^{(n)}(a)\\}^2 + \\{g^{(n)}(a)\\}^2} \\]\n",
      "\n",
      "For \\( n = 1 \\), we have:\n",
      "\n",
      "\\[ f'(a) = e^a (\\cos a + \\sin a) \\]\n",
      "\\[ g'(a) = e^a (\\cos a - \\sin a) \\]\n",
      "\n",
      "For \\( n = 2 \\), we have:\n",
      "\n",
      "\\[ f''(a) = 2e^a \\cos a \\]\n",
      " \n",
      "Extracted:\n",
      "None\n",
      "********************Question:\n",
      "How many distinct ordered pairs of positive integers $(m,n)$ are there so that the sum of the reciprocals of $m$ and $n$ is $\\frac14$? \n",
      "Answer:\n",
      "5 \n",
      "Response:\n",
      "Okay, so I need to find the number of distinct ordered pairs of positive integers (m, n) where 1/m + 1/n = 1/4. Hmm, let's start by trying to rewrite the equation in a more familiar form. If I combine the left side, I get (m + n)/(mn) = 1/4. Then, cross-multiplying gives 4(m + n) = mn. Let me rearrange that: mn - 4m - 4n = 0. Now, to make this solvable, I can add 16 to both sides to complete the rectangle, which is a bit like factoring. So, adding 16: mn - 4m - 4n + 16 = 16. That can be factored as (m - 4)(n - 4) = 16. \n",
      "\n",
      "So now, the problem reduces to finding all pairs of positive integers (m, n) where (m - 4)(n - 4) = 16. Since m and n are positive integers, m - 4 and n - 4 must be positive divisors of 16. The positive divisors of 16 are 1, 2, 4, 8, 16. For each divisor d, we can set m - 4 = d and n - 4 = 16/d, which will give a valid solution. \n",
      "\n",
      "Let's list them out:\n",
      "1. If d = 1, then m = 5, n = 20. So (5, 20) is a solution.\n",
      "2. If d = 2, then m = 6, n = 10. So (6, 10) is a solution.\n",
      "3. If d = 4, then m = 8, n = 6. So (8, 6) is a solution.\n",
      "4. If d = 8, then m = 12, n = 5. So (12, 5) is a solution.\n",
      "5. If d = 16, then m = 20, n = 2. So (20, 2) is a solution.\n",
      "\n",
      "Each of these gives a distinct ordered pair. So the total number of solutions is 5. \n",
      "\n",
      "Wait, but let me check if I missed any. The divisors of 16 are 1, 2, 4, 8, 16, so that's 5 divisors. Since for each divisor, we get a unique pair (m, n), the total should be 5. But let me make sure I didn't make a mistake in the factoring step. The original equation was 4(m + n) = mn, which becomes mn - 4m - 4n = 0. Adding 16: mn -4m -4n +16 =16, which factors to (m-4)(n-4) =16. That seems right. So yes, the number of solutions is 5. So the answer is 5.\n",
      "To find the number of distinct ordered pairs of positive integers \\((m, n)\\) such that \\(\\frac{1}{m} + \\frac{1}{n} = \\frac{1}{4}\\), we start by rewriting the equation in a more convenient form. We have:\n",
      "\n",
      "\\[\n",
      "\\frac{1}{m} + \\frac{1}{n} = \\frac{1}{4}\n",
      "\\]\n",
      "\n",
      "Combining the fractions on the left side, we get:\n",
      "\n",
      "\\[\n",
      "\\frac{m + n}{mn} = \\frac{1}{4}\n",
      "\\]\n",
      "\n",
      "Cross-multiplying to clear the fraction, we obtain:\n",
      "\n",
      "\\[\n",
      "4(m + n) = mn\n",
      "\\]\n",
      "\n",
      "Rearranging terms, we get:\n",
      "\n",
      "\\[\n",
      "mn - 4m - 4n = 0\n",
      "\\]\n",
      "\n",
      "To factor this equation, we add 16 to both sides:\n",
      "\n",
      "\\[\n",
      "mn - 4m - 4n + 16 = 16\n",
      "\\]\n",
      "\n",
      "This can be factored as:\n",
      "\n",
      "\\[\n",
      "(m - 4)(n - 4) = 16\n",
      "\\]\n",
      "\n",
      "Now, we need to find all pairs of positive integers \\((m - 4, n - 4)\\) that multiply to 16. The positive divisors of 16 are \\(1, 2, 4, 8, 16\\). For each divisor \\(d\\), we set \\(m - 4 = d\\) and \\(n - 4 = \\frac{16}{d}\\), giving us the pairs \\((m, n)\\):\n",
      "\n",
      "1. If \\(d = 1\\), then \\(m - 4 = 1\\) and \\(n - 4 = 16\\), so \\(m = 5\\) and \\(n = 20\\). This gives the pair \\((5, 20)\\).\n",
      "2. If \\(d = 2\\), then \\(m - 4 = 2\\) and \\(n - 4 = 8\\), so \\(m = 6\\) and \\(n = 10\\). This gives the pair \\((6, 10)\\).\n",
      "3. If \\(d = 4\\), then \\(m - 4 = 4\\) and \\(n - 4 = 4\\), so \\(m = 8\\) and \\(n = 8\\). This gives the pair \\((8, 8)\\).\n",
      "4. If \\(d = 8\\), then \\(m - 4 = 8\\) and \\(n - 4 = 2\\), so \\(m = 12\\) and \\(n = 6\\). This gives the pair \\((12, 6)\\).\n",
      "5. If \\(d = 16\\), then \\(m - 4 = 16\\) and \\(n - 4 = 1\\), so \\(m = 20\\) and \\(n = 5\\). This gives the pair \\((20, 5)\\).\n",
      "\n",
      "Thus, the distinct ordered pairs \\((m, n)\\) are \\((5, 20)\\), \\((6, 10)\\), \\((8, 8)\\), \\((12, 6)\\), and \\((20, 5)\\). Therefore, there are 5 distinct ordered pairs.\n",
      "\n",
      "The answer is:\n",
      "\n",
      "\\[\n",
      "\\boxed{5}\n",
      "\\]<end_working_out><SOLUTION>5</SOLUTION> \n",
      "Extracted:\n",
      "5\n",
      "********************Question:\n",
      "Suppose that $n$ persons meet in a meeting, and that each of the persons is acquainted with exactly $8$ others. Any two acquainted persons have exactly $4$ common acquaintances, and any two non-acquainted persons have exactly $2$ common acquaintances. Find all possible values of $n$. \n",
      "Answer:\n",
      "21 \n",
      "Response:\n",
      "Okay, so I have this problem about a meeting of n people where each person knows exactly 8 others. The conditions are a bit tricky, but let's try to break it down step by step.\n",
      "\n",
      "First, I need to model this situation. It seems like a graph theory problem. If I think of each person as a vertex in a graph, and an acquaintance as an edge between two vertices, then the problem is about a specific type of graph. The conditions given are about the number of common acquaintances for different pairs of people. \n",
      "\n",
      "So, the problem states:\n",
      "- Each person (vertex) has exactly 8 edges (acquaintances). So, the graph is 8-regular.\n",
      "- For any two people who are acquainted (connected by an edge), they have exactly 4 common acquaintances.\n",
      "- For any two people who are not acquainted, they have exactly 2 common acquaintances.\n",
      "\n",
      "Hmm, this sounds like a graph with specific properties. Maybe a strongly regular graph? Let me recall the definition. A strongly regular graph is a k-regular graph where every pair of adjacent vertices has the same number of common neighbors, and every pair of non-adjacent vertices has the same number of common neighbors. \n",
      "\n",
      "But in this case, the second condition is a bit different. The second condition is that for non-adjacent pairs, the number of common neighbors is 2, not the same as the number for adjacent pairs. So maybe this is a different type of graph. Let me think.\n",
      "\n",
      "Wait, the problem is actually a well-known example in combinatorics. The problem is about a specific type of graph called a \"friendship graph\" or a \"cage\" but I think it's more specific. The correct name is a \"Petersen graph\" or a \"Hajos graph\" but I'm not sure. Let me check the conditions.\n",
      "\n",
      "The condition is that for any two vertices, the number of common neighbors is either 4 (if they are connected) or 2 (if not). This is the definition of a strongly regular graph. So, the graph is a (3,2,0) graph, but wait, that's not right. Let me think again.\n",
      "\n",
      "Let me recall the parameters of a strongly regular graph. A strongly regular graph is denoted as (n, k, Œª, Œº), where n is the number of vertices, k is the degree of each vertex, Œª is the number of common neighbors of any two adjacent vertices, and Œº is the number of common neighbors of any two non-adjacent vertices. So, in this problem, k=8, Œª=4, and Œº=2. But hold on, the problem says that for two non-adjacent vertices, they have 2 common neighbors, so Œº=2. But in the problem statement, it's the other way around. Let me re-examine the problem.\n",
      "\n",
      "The problem says: \"any two non-acquainted persons have exactly 2 common acquaintances.\" So, for non-adjacent vertices, the number of common neighbors is 2. So, the parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not the case. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. Let me think.\n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the graph is a (3,2,0) graph, but that's not a known graph. Wait, no, the correct parameters are n, 8, 4, 2. So, the graph is a (3,2,0) graph, but that's not a known graph. \n",
      "\n",
      "Wait, the problem is actually a different one. The problem is about a graph where each vertex has degree 8, any two adjacent vertices have 4 common neighbors, and any two non-adjacent vertices have 2 common neighbors. This is a specific type of graph. The problem is to find all possible n. So, the \n",
      "Extracted:\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=0.00010830584700064113, metrics={'train_runtime': 12864.0568, 'train_samples_per_second': 0.009, 'train_steps_per_second': 0.002, 'total_flos': 0.0, 'train_loss': 0.00010830584700064113})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# For optional training + evaluation\n",
    "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
    "\n",
    "rl_trainer = GRPOTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    reward_funcs = [\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args = training_args,\n",
    "    train_dataset = rl_dataset,\n",
    "\n",
    "    # For optional training + evaluation\n",
    "    # train_dataset = new_dataset[\"train\"],\n",
    "    # eval_dataset = new_dataset[\"test\"],\n",
    ")\n",
    "rl_trainer.train()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "46cf978f",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f55bc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 395.39it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:23<00:00, 23.10s/it, est. speed input: 0.43 toks/s, output: 44.33 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' - Answers\\nAlgebra\\nWhat is the sqrt of 101?\\nWiki User\\n‚àô 2007-02-01 00:00:00\\nSee answers (2)\\nBest Answer\\nCopy\\n10.0498756211208900000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Êé®ÁêÜ\n",
    "\n",
    "text = \"What is the sqrt of 101?\"\n",
    "\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 1024,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = None,\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd530d",
   "metadata": {},
   "source": [
    "‰ªÖ‰øùÁïôlora ÂæÆË∞ÉÂêéÂèÇÊï∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efe81a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")\n",
    "from safetensors import safe_open\n",
    "\n",
    "tensors = {}\n",
    "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
    "    # Verify both A and B are non zero\n",
    "    for key in f.keys():\n",
    "        tensor = f.get_tensor(key)\n",
    "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
    "        assert(n_zeros.item() != tensor.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183d899c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 1135.13it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:50<00:00, 51.00s/it, est. speed input: 1.39 toks/s, output: 38.77 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, so I need to find the square root of 101. Hmm, let's see. I know that the square root of a number is a value that, when multiplied by itself, gives the original number. But 101 is a bit trickier because it's not a perfect square. \\n\\nFirst, I should think of the perfect squares around 101. The square of 10 is 100, and the square of 11 is 121. So, the square root of 101 must be between 10 and 11. That's a good start. But I need a more precise value. \\n\\nSince 101 is between 100 and 121, the square root of 101 is between 10 and 11. To get a better estimate, maybe I can use a method like the average of 10 and 11, and then check. Let's try 10.05. If I square 10.05, that's 10.05 * 10.05. Let me calculate that. 10*10=100, 0.05*0.05=0.0025, and 10*0.05*2=0.1. So, 100 + 0.1 = 100.1, then add 0.0025, which is 100.1025. That's a bit more than 101. So, 10.05 is too high. \\n\\nNext, try 10.03. Let's square 10.03. 10*10=100, 0.03*0.03=0.0009, 10*0.03*2=0.06. So, 100 + 0.06 = 100.06, add 0.0009, which is 100.0609. That's still a bit high. So, 10.03 is also too high. \\n\\nNext, try 10.02. Let's square 10.02. 10*10=100, 0.02*0.02=0.0004, 10*0.02*2=0.04. So, 100 + 0.04 = 100.04, add 0.0004, which is 100.0404. That's even higher. So, 10.02 is still too high. \\n\\nNext, try 10.01. Let's square 10.01. 10*10=100, 0.01*0.01=0.0001, 10*0.01*2=0.02. So, 100 + 0.02 = 100.02, add 0.0001, which is 100.0201. That's still over 101. So, 10.01 is too high. \\n\\nNext, try 10.005. Let's square 10.005. 10*10=100, 0.005*0.005=0.000025, 10*0.005*2=0.01. So, 100 + 0.01 = 100.01, add 0.000025, which is 100.010025. That's even higher. So, 10.005 is still too high. \\n\\nNext, try 10.001. Let's square 10.001. 10*10=100, 0.001*0.001=0.000001, 10*0.001*2=0.002. So, 100 + 0.002 = 100.002, add 0.000001, which is 100.002001. That's even higher. So, 10.001 is still too high. \\n\\nWait, this is getting very close. The next number I can try is 10.0001. Let's square 10.0001. 10*10=100, 0.0001*0.0001=0.00000001, 10*0.0001*2=0.0002. So, 100 + 0.0002 = 100.0002, add 0.00000001, which is 100.00020001. That's just a little more than 101. So, 10.0001 is way high. \\n\\nSo, the square root of 101 is between 10.00000001 and 10.00000000. But that's not very helpful. Maybe I need a different method. \\n\\nLet me try the long division method for square roots. But I think for 101, the answer is just a little more than 10. So, let's try 10.04. Let's square 10.04. 10*10=100, 0.04*0.04=0.0016, 10*0.04*2=0.08. So, 100 + 0.08 = 100.08, add 0.0016, which is 100.0816. That's a bit more than 101. So, 10.04 is too high. \\n\\nNext, try 10.032. Let's square 10.032. 10*10=100, 0.032*0.032=0.001024, 10*0.032*2=0.064. So, 100 + 0.064 = 100.064, add 0.001024, which is 100.065024. That's even higher. So, 10.032 is too high. \\n\\nNext, try 10.025. Let's square 10.025. 10*10=100, 0.025*0.025=0.000625, 10*0.025*2=0.05. So, 100 + 0.05 = 100.05, add 0.000625, which is 100.050625. That's just a little more than 101. So, 10.025 is still high. \\n\\nNext, try 10.012. Let's square 10.012. 10*10=100, 0.012*0.012=0.000144, 10*0.012*2=0.024. So, 100 + 0.024 = 100.024, add 0.000144, which is 100.024144. That's even higher. So, 10.012 is too high. \\n\\nThis is getting complicated. Maybe I can use a calculator. If I type 101 into a calculator, the square root is approximately 10.0498756. So, the square root of 101 is about 10.0498756. But the question is probably asking for a decimal approximation. So, to one decimal place, that's 10.0. To two decimal places, 10.05. But from my earlier tries, 10.0498756 is closer. So, the more accurate result is 10.0498756. But since the\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test loaded lora\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = model.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55726a8d",
   "metadata": {},
   "source": [
    "# Inference with vLLM in GGUF format\n",
    "\n",
    "**GGUF Ê†ºÂºè**\n",
    "\n",
    "‰∏ãÈù¢‰ª£Á†ÅÊàë‰ª¨Â∞ÜtorchÁöÑsafetensors checkpointÊñá‰ª∂ËΩ¨Êàêgguf Âä†Âø´Ê®°ÂûãÊé®ÁêÜÂä†ÈÄü„ÄÇÈÇ£‰ªÄ‰πàÊòØGGUFÊñá‰ª∂Âë¢Ôºü\n",
    "\n",
    "GGUFÔºàGPT-Generated Unified FormatÔºâÊòØ‰∏ÄÁßç‰∏ì‰∏∫È´òÊïàÈÉ®ÁΩ≤ÂíåÊé®ÁêÜÂ§ßËØ≠Ë®ÄÊ®°ÂûãËÄåËÆæËÆ°ÁöÑ‰∫åËøõÂà∂Êñá‰ª∂Ê†ºÂºèÔºåÁî± llama.cpp È°πÁõÆÁöÑÂàõÂßã‰∫∫ Georgi Gerganov ÊèêÂá∫ÔºåÊòØ GGML Ê†ºÂºèÁöÑÁªß‰ªªËÄÖ „ÄÇ\n",
    "‚úÖ GGUF ÁöÑÊ†∏ÂøÉÁâπÁÇπÔºö\n",
    "ÂçïÊñá‰ª∂ÊâìÂåÖ\n",
    "GGUF Â∞ÜÊ®°ÂûãÁöÑÊùÉÈáç„ÄÅÂàÜËØçÂô®„ÄÅË∂ÖÂèÇÊï∞„ÄÅÊèêÁ§∫Ê®°ÊùøÁ≠âÊâÄÊúâÂÖÉÊï∞ÊçÆÊâìÂåÖËøõ‰∏Ä‰∏™ .gguf Êñá‰ª∂‰∏≠ÔºåÈÉ®ÁΩ≤Êó∂Êó†ÈúÄÈ¢ùÂ§ñÈÖçÁΩÆÊñá‰ª∂ÔºåÊûÅÂ§ßÁÆÄÂåñ‰∫ÜÊ®°ÂûãÂàÜÂèë‰∏é‰ΩøÁî® „ÄÇ\n",
    "ÊîØÊåÅÈáèÂåñ\n",
    "GGUF ÂéüÁîüÊîØÊåÅÂ§öÁßçÈáèÂåñÊñπÂºèÔºàÂ¶Ç Q4_K_M„ÄÅQ5_1„ÄÅQ8_0 Á≠âÔºâÔºåÂèØ‰ª•Â∞ÜÊ®°Âûã‰ΩìÁßØ‰ªéÂçÅÂá† GB ÂéãÁº©Âà∞Âá† GBÔºåÈÄÇÂêàÂú®Ê∂àË¥πÁ∫ßÁ°¨‰ª∂‰∏äËøêË°å „ÄÇ\n",
    "Ë∑®Âπ≥Âè∞ÂÖºÂÆπÊÄßÂº∫\n",
    "GGUF Ê†ºÂºèË¢´ËÆæËÆ°‰∏∫Ë∑®Âπ≥Âè∞ÈÄöÁî®ÔºåÊîØÊåÅ Windows„ÄÅLinux„ÄÅmacOSÔºåÁîöËá≥ÂèØ‰ª•Âú®ÊâãÊú∫ÊàñËæπÁºòËÆæÂ§á‰∏äËøêË°åÔºåÈÄÇÈÖç CPU„ÄÅCUDA„ÄÅMetal„ÄÅVulkan Á≠âÂ§öÁßçÂêéÁ´Ø „ÄÇ\n",
    "Âä†ËΩΩÂø´„ÄÅÊé®ÁêÜÊïàÁéáÈ´ò\n",
    "GGUF ‰ΩøÁî®ÂÜÖÂ≠òÊò†Â∞ÑÔºàmmapÔºâÊäÄÊúØÔºåÊîØÊåÅÂø´ÈÄüÂä†ËΩΩÊ®°ÂûãÔºåÁâπÂà´ÈÄÇÂêàÊú¨Âú∞Êé®ÁêÜÂú∫ÊôØ „ÄÇ\n",
    "ÂèØÊâ©Â±ïÊÄßÂº∫\n",
    "GGUF ‰ΩøÁî®ÈîÆÂÄºÂØπÁªìÊûÑÂ≠òÂÇ®ÂÖÉÊï∞ÊçÆÔºåÊîØÊåÅÂú®‰∏çÁ†¥ÂùèÂÖºÂÆπÊÄßÁöÑÂâçÊèê‰∏ãÊ∑ªÂä†Êñ∞‰ø°ÊÅØÔºåÂÖ∑Â§áËâØÂ•ΩÁöÑÊú™Êù•ÈÄÇÂ∫îÊÄß „ÄÇ\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283676ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/ai_project/AlgorithmCodingPractice/SFT\n"
     ]
    }
   ],
   "source": [
    "# !pwd\n",
    "# !cd ../ && git clone https://github.com/ggerganov/llama.cpp.git\n",
    "# !cd ../llama.cpp/ && python -m pip install -r requirements.txt\n",
    "#! cd ../llama.cpp/ && python convert.py --model ./path_to_your_model_directory --outfile ./path_to_your_model.gguf --quantize q4_k_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f55c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 17:16:11 [interface.py:409] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 11-30 17:16:19 [vllm_utils.py:702] Unsloth: Patching vLLM v1 graph capture\n",
      "==((====))==  Unsloth 2025.11.4: Fast Qwen3 patching. Transformers: 4.57.2. vLLM: 0.11.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5070 Laptop GPU. Num GPUs = 1. Max memory: 7.96 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/qwen3-4b-base-unsloth-bnb-4bit with actual GPU utilization = 50.65%\n",
      "Unsloth: Your GPU has CUDA compute capability 12.0 with VRAM = 7.96 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 128.\n",
      "Unsloth: vLLM's KV Cache can use up to 1.28 GB. Also swap space = 0 GB.\n",
      "Unsloth: Not an error, but `use_cudagraph` is not supported in vLLM.config.CompilationConfig. Skipping.\n",
      "WARNING 11-30 17:16:43 [compilation.py:610] Level is deprecated and will be removed in the next release,either 0.12.0 or 0.11.2 whichever is soonest.Use mode instead.If both level and mode are given,only mode will be used.\n",
      "WARNING 11-30 17:16:43 [compilation.py:699] The 'use_inductor' flag is deprecated and will be removed in the next release (v0.12.0). Please use the 'backend' option instead.\n",
      "Unsloth: Not an error, but `device` is not supported in vLLM. Skipping.\n",
      "INFO 11-30 17:16:43 [utils.py:253] non-default args: {'load_format': 'bitsandbytes', 'dtype': torch.bfloat16, 'seed': 0, 'max_model_len': 1024, 'enable_prefix_caching': True, 'swap_space': 0, 'gpu_memory_utilization': 0.506468111863445, 'max_num_batched_tokens': 2048, 'max_num_seqs': 128, 'max_logprobs': 0, 'disable_log_stats': True, 'quantization': 'bitsandbytes', 'enable_lora': True, 'max_lora_rank': 32, 'enable_chunked_prefill': True, 'compilation_config': {'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': [], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': True, 'compile_sizes': None, 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 32, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': None, 'local_cache_dir': None}, 'model': 'unsloth/qwen3-4b-base-unsloth-bnb-4bit'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
      "  return self.serializer.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:16:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 17:16:51 [model.py:1745] Using max model len 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 17:16:51,764\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:16:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'bfloat16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.0.mlp', 'model.layers.4.mlp', 'model.layers.3.self_attn', 'model.layers.0.self_attn', 'model.layers.6.mlp', 'model.layers.1.self_attn', 'model.layers.1.mlp', 'model.layers.2.mlp'], 'llm_int8_threshold': 6.0}\n",
      "INFO 11-30 17:16:59 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='unsloth/qwen3-4b-base-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/qwen3-4b-base-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1024, download_dir=None, load_format=bitsandbytes, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/qwen3-4b-base-unsloth-bnb-4bit, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': 3, 'mode': 3, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': True, 'compile_sizes': [], 'inductor_compile_config': {'epilogue_fusion': True, 'max_autotune': False, 'shape_padding': True, 'trace.enabled': False, 'triton.cudagraphs': False, 'debug': False, 'dce': True, 'memory_planning': True, 'coordinate_descent_tuning': False, 'trace.graph_diagram': False, 'compile_threads': 32, 'group_fusion': True, 'disable_progress': False, 'verbose_progress': True, 'triton.multi_kernel': 0, 'triton.use_block_ptr': True, 'triton.enable_persistent_tma_matmul': True, 'triton.autotune_at_compile_time': False, 'triton.cooperative_reductions': False, 'cuda.compile_opt_level': '-O2', 'cuda.enable_cuda_lto': True, 'combo_kernels': False, 'benchmark_combo_kernel': True, 'combo_kernel_foreach_dynamic_shapes': True, 'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 256, 'local_cache_dir': None}\n",
      "INFO 11-30 17:16:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.61.219:43709 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "INFO 11-30 17:16:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/pydantic/type_adapter.py:605: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `enum` - serialized value may not be as expected [field_name='mode', input_value=3, input_type=int])\n",
      "  return self.serializer.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:17:00 [topk_topp_sampler.py:36] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-30 17:17:00 [gpu_model_runner.py:3259] Starting to load model unsloth/qwen3-4b-base-unsloth-bnb-4bit...\n",
      "INFO 11-30 17:17:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "INFO 11-30 17:17:01 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "INFO 11-30 17:17:01 [bitsandbytes_loader.py:791] Loading weights with BitsAndBytes quantization. May take a while ...\n",
      "INFO 11-30 17:17:07 [weight_utils.py:441] Time spent downloading weights for unsloth/qwen3-4b-base-unsloth-bnb-4bit: 1.286020 seconds\n",
      "INFO 11-30 17:17:08 [weight_utils.py:481] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.59it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.58it/s]\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.15s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:17:12 [punica_selector.py:20] Using PunicaWrapperGPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:17:12 [gpu_model_runner.py:3338] Model loading took 3.2369 GiB memory and 11.451242 seconds\n",
      "INFO 11-30 17:17:22 [backends.py:631] Using cache directory: /home/wwk/.cache/vllm/torch_compile_cache/67d3f6e294/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 11-30 17:17:22 [backends.py:647] Dynamo bytecode transform time: 9.53 s\n",
      "INFO 11-30 17:17:25 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.752 s\n",
      "INFO 11-30 17:17:27 [monitor.py:34] torch.compile takes 12.28 s in total\n",
      "INFO 11-30 17:17:29 [gpu_worker.py:359] Available KV cache memory: 0.51 GiB\n",
      "INFO 11-30 17:17:29 [kv_cache_utils.py:1229] GPU KV cache size: 3,696 tokens\n",
      "INFO 11-30 17:17:29 [kv_cache_utils.py:1234] Maximum concurrency for 1,024 tokens per request: 3.61x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 17:17:29,464 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-30 17:17:29,715 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:17:29 [vllm_utils.py:707] Unsloth: Running patched vLLM v1 `capture_model`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 17:17:30 [utils.py:250] Using default LoRA kernel configs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 70/70 [00:13<00:00,  5.27it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 38/38 [00:09<00:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:17:52 [gpu_model_runner.py:4244] Graph capturing finished in 21 secs, took 1.96 GiB\n",
      "INFO 11-30 17:17:52 [vllm_utils.py:714] Unsloth: Patched vLLM v1 graph capture finished in 21 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 17:17:54 [core.py:250] init engine (profile, create kv cache, warmup model) took 41.46 seconds\n",
      "INFO 11-30 17:17:58 [llm.py:352] Supported tasks: ('generate',)\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'post_layernorm', 'norm2', 'ffn_norm', 'layer_norm2', 'attention_norm', 'norm1', 'post_attention_layernorm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'q_norm', 'input_layernorm', 'k_norm', 'norm']\n",
      "Performing substitution for additional_keys=set()\n",
      "Unsloth: Just some info: will skip parsing ['layer_norm1', 'post_layernorm', 'norm2', 'ffn_norm', 'layer_norm2', 'attention_norm', 'cross_attn_input_layernorm', 'cross_attn_post_attention_layernorm', 'norm1', 'post_attention_layernorm', 'post_feedforward_layernorm', 'pre_feedforward_layernorm', 'q_norm', 'input_layernorm', 'k_norm', 'norm']\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 1024 # Can increase for longer reasoning traces\n",
    "lora_rank = 32 # Larger rank = smarter, but slower\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     device_map = \"cpu\",  # Âº∫Âà∂‰ΩøÁî®CPU\n",
    "#     dtype = torch.float16,  # CPU‰∏ä‰ΩøÁî®float16\n",
    "#     load_in_4bit = False, # False for LoRA 16bit when merging model and LoRA\n",
    "#     fast_inference = False, # Enable vLLM fast inference\n",
    "#     gpu_memory_utilization = 0.6, # Reduce if out of memory\n",
    "# )\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True, # False for LoRA 16bit\n",
    "    fast_inference = True, # Enable vLLM fast inference\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.6 # Reduce if out of memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d5c8597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.4 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LoRARequest(lora_name='1', lora_int_id=1, lora_path='grpo_saved_lora', lora_tensors=None, lora_config=(None,), lora_local_path=None, long_lora_max_len=None, base_model_name=None, lora_embeddings=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "pmodel = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "pmodel.load_lora(\"grpo_saved_lora\")\n",
    "# pmodel = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     \"./grpo_saved_lora\",\n",
    "#     # device_map=\"cpu\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b52388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from datasets import load_dataset\n",
    "# rl_dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
    "# rl_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce4f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 17:18:06 [processor.py:246] vLLM has deprecated support for supporting different tokenizers for different LoRAs. By default, vLLM uses base model's tokenizer. If you are using a LoRA with its own tokenizer, consider specifying `--tokenizer [lora_path]` to use the LoRA tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 346.12it/s]\n",
      "Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:21<00:00, 21.59s/it, est. speed input: 2.69 toks/s, output: 44.75 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Okay, so I need to find the square root of 101. Hmm, let's see. I know that the square root of 100 is 10, but 101 is just 1 more than 100. So the square root of 101 should be a little more than 10. But how much more? Let me think.\\n\\nI remember this method where you can estimate the square root by taking the average of the number and its previous perfect square. For example, the square root of 100 is 10, and 100 is 10^2. The next number is 101, so maybe the square root is (100^0.5 + 101/100) or something like that. Let me try that.\\n\\nSo, 100^0.5 is 10, and 101 divided by 100 is 1.01. Then add them: 10 + 1.01 = 11.01. But that can't be right because 11.01 squared is 121.21, which is way more than 101. So that method must be wrong. I need a different approach.\\n\\nWait, maybe I can use the fact that 101 is between 100 and 121, so the square root is between 10 and 11. Let's try 10.05. 10.05 squared is 100.5025, which is still less than 101. So the square root is a bit more than 10.05. How about 10.051? Let's check. 10.051 times 10.051. That's 10.051^2. \\n\\nBut doing the multiplication: 10.051 * 10.051. Let's break it down. 10*10=100, 10*0.051=0.51, 0.051*10=0.51, 0.051*0.051. So 0.002601. Adding them up: 100 + 0.51 + 0.51 = 101.02, plus 0.002601, which is 101.022601. That's a bit more than 101. So 10.051 is a bit high. So the square root is between 10.050 and 10.051. \\n\\nBut I need a more accurate value. Maybe I can use a calculator. But since I need to do this by hand, let's use the long division method for square roots. \\n\\nSo, the long division method. Start with 101.000000... (add as many decimal places as needed for accuracy). The largest number whose square is less than 101 is 10, because 10*10=100. Write 10 above the line, subtract 100 from 101, which leaves 1. Bring down the next 0, making it 10. Now, double the number on top, which is 10, so 20. We need a number 'x' such that 20x * x is as close to 100 as possible. 20*5*5=100, so 5. Add 5 to 10, making 105. But 105*5=525, which is way more than 100. So 10.5. But 10.5*10.5=110.25, which is more than 101. So 10.05. \\n\\nWait, but when I did 10.051, the square was 101.0226, which is more. So 10.050. Let's try 10.050. 10.050*10.050. 10*10=100, 10*0.0\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test loaded lora\n",
    "\n",
    "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
    "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
    "# system_prompt\n",
    "\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace with out specific template:\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer.chat_template = chat_template\n",
    "#test loaded lora\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    tokenize = False,\n",
    ")\n",
    "from vllm import SamplingParams\n",
    "sampling_params = SamplingParams(\n",
    "    temperature = 1.0,\n",
    "    top_k = 50,\n",
    "    max_tokens = 2048,\n",
    ")\n",
    "output = pmodel.fast_generate(\n",
    "    text,\n",
    "    sampling_params = sampling_params,\n",
    "    lora_request = pmodel.load_lora(\"grpo_saved_lora\"),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fbcad28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wwk/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# help(tokenizer)\n",
    "# tokenizer.model_type = 'qwen3'\n",
    "# tokenizer.save_pretrained(\"./sft_qwen3_math_merged_model_bit\")\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "# ÊµãËØïÂä†ËΩΩ\n",
    "# test_model = AutoModel.from_pretrained(\"./model_dir\", trust_remote_code=True)\n",
    "\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\"unsloth/Qwen3-4B-Base\", trust_remote_code=True)\n",
    "\n",
    "test_tokenizer.save_pretrained(\"./sft_qwen3_math_merged_model_bit\", trust_remote_code=True)\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(\"./sft_qwen3_math_merged_model_bit\", trust_remote_code=True)\n",
    "# test_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca1e086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "# os.environ['HF_ENDPOINT'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a72fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/qwen3-4b-base/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7b5665b34910>, 'Connection to 172.22.48.1 timed out. (connect timeout=10)')))\"), '(Request ID: 0a4f90f7-ec51-4b96-b293-f81736de2aed)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /unsloth/qwen3-4b-base/resolve/main/config.json (Caused by ProxyError('Unable to connect to proxy', ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7b5665b34910>, 'Connection to 172.22.48.1 timed out. (connect timeout=10)')))\"), '(Request ID: 0a4f90f7-ec51-4b96-b293-f81736de2aed)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/wwk/.cache/huggingface/hub\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [13:01<00:00, 390.98s/it]\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 46febe43-3955-479b-8c73-2b398d148c6c)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/tokenizer.model\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 46febe43-3955-479b-8c73-2b398d148c6c)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/tokenizer.model\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: tokenizer.model not found (this is OK for non-SentencePiece models)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:34<00:00, 17.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merge process complete. Saved to `/home/wwk/workspace/ai_project/AlgorithmCodingPractice/SFT/sft_qwen3_math_merged_model_bit`\n"
     ]
    }
   ],
   "source": [
    "pmodel.save_pretrained_merged(\n",
    "    save_directory=\"./sft_qwen3_math_merged_model_bit\",\n",
    "    tokenizer=tokenizer,\n",
    "    save_method=\"merged_16bit\",  # Êàñ \"lora\" / \"merged_4bit\" Áúã‰Ω†ÈúÄÊ±Ç\n",
    "    push_to_hub=False,  # ÊòæÂºèÂÖ≥Èó≠‰∏ä‰º†\n",
    "    safe_serialization=True,\n",
    "    max_shard_size=\"5GB\",  # ÂèØÈÄâÔºöÈò≤Ê≠¢ÂçïÊñá‰ª∂ËøáÂ§ß\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf1c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b2a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./sft_qwen3_math_merged_model_16bit/tokenizer_config.json',\n",
       " './sft_qwen3_math_merged_model_16bit/special_tokens_map.json',\n",
       " './sft_qwen3_math_merged_model_16bit/vocab.json',\n",
       " './sft_qwen3_math_merged_model_16bit/merges.txt',\n",
       " './sft_qwen3_math_merged_model_16bit/added_tokens.json',\n",
       " './sft_qwen3_math_merged_model_16bit/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.save_pretrained(\"./sft_qwen3_math_merged_model_16bit\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60eda2",
   "metadata": {},
   "source": [
    "‰ΩøÁî®unsloth Ëá™Â∏¶ÁöÑÂÆâË£Öllama ggufÂåÖÁöÑÊé•Âè£ÂéªÈáèÂåñÂíå‰øùÂ≠òÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d65aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging model weights to 16-bit format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 11292d35-d69d-4419-952f-c1ec98812de4)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 11292d35-d69d-4419-952f-c1ec98812de4)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 0fb305e0-a1cd-4dca-b3b4-5588e45bdc15)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 0fb305e0-a1cd-4dca-b3b4-5588e45bdc15)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ea000631-689d-4800-b8a8-b7b3a23cab1d)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: ea000631-689d-4800-b8a8-b7b3a23cab1d)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 1cb817c8-88d8-49a3-9faf-2336cab37bc8)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 1cb817c8-88d8-49a3-9faf-2336cab37bc8)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: acf2f3f8-3745-4776-b800-3241a25b9d10)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: acf2f3f8-3745-4776-b800-3241a25b9d10)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: b25d3fcd-e2d7-484f-9f7e-bd84b7be7445)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: b25d3fcd-e2d7-484f-9f7e-bd84b7be7445)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/wwk/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Returning existing local_dir `sft_qwen3_math_gguf_model_q8` as remote repo cannot be accessed in `snapshot_download` ((ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: f14e7fdc-1b4c-47d8-8a51-5016627ee8a0)')).\n",
      "[huggingface_hub._snapshot_download|WARNING]Returning existing local_dir `sft_qwen3_math_gguf_model_q8` as remote repo cannot be accessed in `snapshot_download` ((ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: f14e7fdc-1b4c-47d8-8a51-5016627ee8a0)')).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00002.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n",
      "Checking cache directory for required files...\n",
      "Cache check failed: tokenizer.model not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:00<?, ?it/s]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 7741ab77-c063-4950-823a-5e3da46003f8)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 7741ab77-c063-4950-823a-5e3da46003f8)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: cc81f549-0072-4590-a52e-56d2d84b34a2)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: cc81f549-0072-4590-a52e-56d2d84b34a2)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "Retrying in 2s [Retry 2/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: d90dcefa-fcd4-47c3-87c6-ec8078a683a6)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: d90dcefa-fcd4-47c3-87c6-ec8078a683a6)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "Retrying in 4s [Retry 3/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 6cd98dd1-ee01-443b-adde-3845939d030f)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 6cd98dd1-ee01-443b-adde-3845939d030f)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "Retrying in 8s [Retry 4/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 388e1fb2-cd35-4913-95c5-1d1368e57d1c)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: 388e1fb2-cd35-4913-95c5-1d1368e57d1c)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "Retrying in 8s [Retry 5/5].\n",
      "[huggingface_hub.utils._http|WARNING]Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: d65bfd90-326a-4c37-94db-813147564a02)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "[huggingface_hub.utils._http|WARNING]'(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: d65bfd90-326a-4c37-94db-813147564a02)')' thrown while requesting HEAD https://huggingface.co/unsloth/qwen3-4b-base/resolve/main/model-00001-of-00002.safetensors\n",
      "Unsloth: Preparing safetensor model files:   0%|          | 0/2 [00:53<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to save/merge model: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connectionpool.py:773\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m, SocketTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connectionpool.py:1042\u001b[39m, in \u001b[36mHTTPSConnectionPool._prepare_proxy\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1036\u001b[39m conn.set_tunnel(\n\u001b[32m   1037\u001b[39m     scheme=tunnel_scheme,\n\u001b[32m   1038\u001b[39m     host=\u001b[38;5;28mself\u001b[39m._tunnel_host,\n\u001b[32m   1039\u001b[39m     port=\u001b[38;5;28mself\u001b[39m.port,\n\u001b[32m   1040\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.proxy_headers,\n\u001b[32m   1041\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/util/ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/util/ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mProtocolError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/util/retry.py:474\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_method_retryable(method):\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/util/util.py:38\u001b[39m, in \u001b[36mreraise\u001b[39m\u001b[34m(tp, value, tb)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m value.__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m value.with_traceback(tb)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connectionpool.py:773\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_proxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m, SocketTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connectionpool.py:1042\u001b[39m, in \u001b[36mHTTPSConnectionPool._prepare_proxy\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1036\u001b[39m conn.set_tunnel(\n\u001b[32m   1037\u001b[39m     scheme=tunnel_scheme,\n\u001b[32m   1038\u001b[39m     host=\u001b[38;5;28mself\u001b[39m._tunnel_host,\n\u001b[32m   1039\u001b[39m     port=\u001b[38;5;28mself\u001b[39m.port,\n\u001b[32m   1040\u001b[39m     headers=\u001b[38;5;28mself\u001b[39m.proxy_headers,\n\u001b[32m   1041\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1042\u001b[39m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connection.py:790\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m server_hostname_rm_dot = server_hostname.rstrip(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m sock_and_verified = \u001b[43m_ssl_wrap_socket_and_match_hostname\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_minimum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_maximum_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname_rm_dot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43massert_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock_and_verified.socket\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/connection.py:969\u001b[39m, in \u001b[36m_ssl_wrap_socket_and_match_hostname\u001b[39m\u001b[34m(sock, cert_reqs, ssl_version, ssl_minimum_version, ssl_maximum_version, cert_file, key_file, key_password, ca_certs, ca_cert_dir, ca_cert_data, assert_hostname, assert_fingerprint, server_hostname, ssl_context, tls_in_tls)\u001b[39m\n\u001b[32m    967\u001b[39m         server_hostname = normalized\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m ssl_sock = \u001b[43mssl_wrap_socket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeyfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcertfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_password\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_certs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    976\u001b[39m \u001b[43m    \u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mca_cert_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    978\u001b[39m \u001b[43m    \u001b[49m\u001b[43mssl_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/util/ssl_.py:480\u001b[39m, in \u001b[36mssl_wrap_socket\u001b[39m\u001b[34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[39m\n\u001b[32m    478\u001b[39m context.set_alpn_protocols(ALPN_PROTOCOLS)\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m ssl_sock = \u001b[43m_ssl_wrap_socket_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtls_in_tls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ssl_sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/urllib3/util/ssl_.py:524\u001b[39m, in \u001b[36m_ssl_wrap_socket_impl\u001b[39m\u001b[34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[39m\n\u001b[32m    522\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mssl_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrap_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/ssl.py:455\u001b[39m, in \u001b[36mSSLContext.wrap_socket\u001b[39m\u001b[34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_socket\u001b[39m(\u001b[38;5;28mself\u001b[39m, sock, server_side=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    450\u001b[39m                 do_handshake_on_connect=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    451\u001b[39m                 suppress_ragged_eofs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    452\u001b[39m                 server_hostname=\u001b[38;5;28;01mNone\u001b[39;00m, session=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# ctx._wrap_socket()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msslsocket_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43msock\u001b[49m\u001b[43m=\u001b[49m\u001b[43msock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_handshake_on_connect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuppress_ragged_eofs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mserver_hostname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[43msession\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/ssl.py:1076\u001b[39m, in \u001b[36mSSLSocket._create\u001b[39m\u001b[34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[39m\n\u001b[32m   1075\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1076\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/ssl.py:1372\u001b[39m, in \u001b[36mSSLSocket.do_handshake\u001b[39m\u001b[34m(self, block)\u001b[39m\n\u001b[32m   1371\u001b[39m         \u001b[38;5;28mself\u001b[39m.settimeout(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1372\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_handshake\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mProtocolError\u001b[39m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1542\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/file_download.py:306\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m hf_raise_for_status(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:325\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nb_tries > max_retries:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    327\u001b[39m \u001b[38;5;66;03m# Sleep for X seconds\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:306\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m response = \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m retry_on_status_codes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:95\u001b[39m, in \u001b[36mUniqueRequestIdAdapter.send\u001b[39m\u001b[34m(self, request, *args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/requests/adapters.py:659\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionError\u001b[39m: (ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')), '(Request ID: d65bfd90-326a-4c37-94db-813147564a02)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mLocalEntryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/unsloth/save.py:1896\u001b[39m, in \u001b[36munsloth_save_pretrained_gguf\u001b[39m\u001b[34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   1894\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1895\u001b[39m     \u001b[38;5;66;03m# Call unsloth_generic_save directly (it's in the same file)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1896\u001b[39m     \u001b[43munsloth_generic_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/unsloth/save.py:2636\u001b[39m, in \u001b[36munsloth_generic_save\u001b[39m\u001b[34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   2634\u001b[39m     save_method = \u001b[33m\"\u001b[39m\u001b[33mmerged_4bit\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2636\u001b[39m \u001b[43mmerge_and_overwrite_lora\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2638\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2639\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2640\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2641\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2642\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_disk_space_usage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_temp_file\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/unsloth_zoo/saving_utils.py:1276\u001b[39m, in \u001b[36mmerge_and_overwrite_lora\u001b[39m\u001b[34m(get_model_name, model, tokenizer, save_directory, push_to_hub, private, token, save_method, output_dtype, low_disk_space_usage, use_temp_file, cleanup_temp_file)\u001b[39m\n\u001b[32m   1275\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m copied_all_from_cache \u001b[38;5;129;01mand\u001b[39;00m low_disk_space_usage \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(file_path) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_path:\n\u001b[32m-> \u001b[39m\u001b[32m1276\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1281\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1282\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/file_download.py:987\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    979\u001b[39m         warnings.warn(\n\u001b[32m    980\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`local_dir_use_symlinks` parameter is deprecated and will be ignored. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    981\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe process to download files to a local folder has been updated and do \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    984\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFor more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    985\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_local_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/file_download.py:1250\u001b[39m, in \u001b[36m_hf_hub_download_to_local_dir\u001b[39m\u001b[34m(local_dir, repo_id, repo_type, filename, revision, endpoint, etag_timeout, headers, proxies, token, cache_dir, force_download, local_files_only)\u001b[39m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m# Otherwise => raise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/huggingface_hub/file_download.py:1658\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[32m   1659\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is on.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1662\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhead_call_error\u001b[39;00m\n",
      "\u001b[31mLocalEntryNotFoundError\u001b[39m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ÂØºÂá∫ggufÊ†ºÂºèÈáèÂåñÂêéÊ®°Âûã\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# model2 = FastLanguageModel.get_peft_model(\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#     model,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#model.save_pretrained_gguf(\"sft_qwen3_math_gguf_model_q4\", tokenizer, quantization_method = \"q4_k_m\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mpmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msft_qwen3_math_gguf_model_q8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mq8_0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# model.save_pretrained_gguf(\"sft_qwen3_math_gguf_model_f16\", tokenizer, quantization_method = \"f16\")\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# model.save_pretrained_gguf(\"sft_qwen3_math_gguf_model_f16_v1\", tokenizer, quantization_method = \"f16\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/miniconda3/envs/llm_study/lib/python3.13/site-packages/unsloth/save.py:1899\u001b[39m, in \u001b[36munsloth_save_pretrained_gguf\u001b[39m\u001b[34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[39m\n\u001b[32m   1896\u001b[39m     unsloth_generic_save(**arguments)\n\u001b[32m   1898\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1899\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to save/merge model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_processor:\n\u001b[32m   1902\u001b[39m     tokenizer = tokenizer.tokenizer\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to save/merge model: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "# ÂØºÂá∫ggufÊ†ºÂºèÈáèÂåñÂêéÊ®°Âûã\n",
    "# model2 = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     # r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "#     # target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     # lora_alpha = lora_rank*2, # *2 speeds up training\n",
    "#     # use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
    "#     # random_state = 3407,\n",
    "# )\n",
    "\n",
    "#model.save_pretrained_gguf(\"sft_qwen3_math_gguf_model_q4\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "pmodel.save_pretrained_gguf(\"sft_qwen3_math_gguf_model_q8\", tokenizer, quantization_method = \"q8_0\")\n",
    "# model.save_pretrained_gguf(\"sft_qwen3_math_gguf_model_f16\", tokenizer, quantization_method = \"f16\")\n",
    "# model.save_pretrained_gguf(\"sft_qwen3_math_gguf_model_f16_v1\", tokenizer, quantization_method = \"f16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580073d6",
   "metadata": {},
   "source": [
    "ÊâãÂä®‰ΩøÁî® llama.cpp ‰ªìÂ∫ìÁöÑconvert_hf_to_gguf.py ËÑöÊú¨ÂéªÈáèÂåñÊ®°ÂûãÊàêgguf 8bitÁöÑÊ†ºÂºè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bcabbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: sft_qwen3_math_merged_model_bit\n",
      "INFO:hf-to-gguf:Model architecture: Qwen3ForCausalLM\n",
      "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00002.safetensors'\n",
      "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00002.safetensors'\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:token_embd.weight,         torch.bfloat16 --> Q8_0, shape = {2560, 151936}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.0.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.1.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.10.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.11.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.12.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.13.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.14.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.15.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.16.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.16.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.16.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.16.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.16.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.16.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.17.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.17.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.17.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.17.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.17.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.17.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.18.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.18.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.18.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.18.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.18.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.18.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.19.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.19.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.19.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.19.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.19.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.19.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.2.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.20.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.20.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.20.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.20.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.3.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.4.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.5.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.6.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.7.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.8.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,     torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,     torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,     torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_k_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,  torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.9.attn_q_norm.weight,  torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,       torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.20.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.20.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.21.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.21.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.21.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.21.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.21.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.21.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.22.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.22.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.22.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.22.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.22.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.22.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.23.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.23.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.23.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.23.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.23.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.23.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.24.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.24.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.24.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.24.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.24.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.24.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.25.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.25.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.25.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.25.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.25.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.25.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.26.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.26.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.26.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.26.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.26.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.26.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.27.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.27.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.27.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.27.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.27.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.27.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.28.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.28.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.28.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.28.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.28.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.28.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.29.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.29.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.29.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.29.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.29.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.29.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.30.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.30.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.30.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.30.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.30.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.30.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.31.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.31.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.31.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.31.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.31.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.31.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.32.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.32.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.32.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.32.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.32.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.32.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.33.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.33.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.33.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.33.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.33.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.33.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.34.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.34.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.34.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.34.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.34.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.34.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_norm.weight,   torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_down.weight,    torch.bfloat16 --> Q8_0, shape = {9728, 2560}\n",
      "INFO:hf-to-gguf:blk.35.ffn_gate.weight,    torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_up.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 9728}\n",
      "INFO:hf-to-gguf:blk.35.ffn_norm.weight,    torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_k_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_k.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:blk.35.attn_output.weight, torch.bfloat16 --> Q8_0, shape = {4096, 2560}\n",
      "INFO:hf-to-gguf:blk.35.attn_q_norm.weight, torch.bfloat16 --> F32, shape = {128}\n",
      "INFO:hf-to-gguf:blk.35.attn_q.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 4096}\n",
      "INFO:hf-to-gguf:blk.35.attn_v.weight,      torch.bfloat16 --> Q8_0, shape = {2560, 1024}\n",
      "INFO:hf-to-gguf:output_norm.weight,        torch.bfloat16 --> F32, shape = {2560}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 32768\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2560\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 9728\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 1000000\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-06\n",
      "INFO:hf-to-gguf:gguf: file type = 7\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 151387 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type eos to 151643\n",
      "INFO:gguf.vocab:Setting special token type pad to 151654\n",
      "INFO:gguf.vocab:Setting add_bos_token to False\n",
      "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ messages[0]['content'] + eos_token }}{% set loop_messages = messages[1:] %}{% else %}{{ 'You are given a problem.\n",
      "Think about the problem and provide your working out.\n",
      "Place it between <start_working_out> and <end_working_out>.\n",
      "Then, provide your solution between <SOLUTION></SOLUTION>' + eos_token }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ message['content'] }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<start_working_out>' }}{% endif %}\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:sft_qwen3_math_gguf_q8.gguf: n_tensors = 398, total_size = 4.3G\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.27G/4.27G [00:29<00:00, 144Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to sft_qwen3_math_gguf_q8.gguf\n"
     ]
    }
   ],
   "source": [
    "#ÊâãÂä®ÈÄöËøállama.cpp ËΩ¨Êç¢Êàê8bit ggufÈáèÂåñÊ†ºÂºè\n",
    "!cd ../llama.cpp/ && python convert_hf_to_gguf.py  ../SFT/sft_qwen3_math_merged_model_bit/ --outfile sft_qwen3_math_gguf_q8.gguf --outtype q8_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35df7825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../llama.cpp/sft_qwen3_math_gguf_q8.gguf\n"
     ]
    }
   ],
   "source": [
    "!ls ../llama.cpp/sft_qwen3_math_gguf_q8.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3e179",
   "metadata": {},
   "source": [
    "### ÂêØÂä®vLLM ÈÉ®ÁΩ≤Ê®°Âûã \n",
    "```shell\n",
    "--tokenizer <tokenizer‰øùÂ≠òË∑ØÂæÑ ÊàñËÄÖ tokenizer from_pretrainÁöÑÊ®°ÂûãÁâàÊú¨ÂêçÁß∞>\n",
    "--gpu-memory-utilization <gpuÊòæÂ≠òÂà©Áî®ÊØî‰æã>\n",
    "--max-model-len 512 <ÈôêÂà∂Âçï‰∏™ËØ∑Ê±ÇÁöÑËæìÂÖ•ÂíåËæìÂá∫‰ª§ÁâåÊÄªÊï∞ÔºàÂç≥‰∏ä‰∏ãÊñáÈïøÂ∫¶Ôºâ> #Ê®°ÂûãÊû∂ÊûÑÊú¨Ë∫´ÔºàÂ¶ÇËÆ≠ÁªÉÊó∂ÁöÑmax_position_embeddingsÔºâÂíåGPUÊòæÂ≠ò\n",
    "--max-num-seqs 2 <ÊéßÂà∂ÂêåÊó∂Â§ÑÁêÜÁöÑËØ∑Ê±ÇÂ∫èÂàóÔºàSequenceÔºâÊï∞Èáè‰∏äÈôê > #‰∏ªË¶ÅÁî±GPUÁöÑKVÁºìÂ≠òÊòæÂ≠òÂ§ßÂ∞èÂÜ≥ÂÆö \n",
    "--max-num-batched-tokens 256 <ÈôêÂà∂È¢ÑÂ°´ÂÖÖÔºàPrefillÔºâÈò∂ÊÆµÔºåÂçïÊâπÊ¨°ÊâÄÊúâËØ∑Ê±ÇÁöÑÊèêÁ§∫ÔºàPromptÔºâ‰ª§ÁâåÊÄªÊï∞‰∏äÈôê > #GPUÊòæÂ≠òÂíåËÆ°ÁÆóÊÄßËÉΩ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26492334",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../llama.cpp/\n",
    "!vllm serve sft_qwen3_math_gguf_q8.gguf --tokenizer ../SFT/sft_qwen3_math_merged_model_bit --gpu-memory-utilization 0.6 --max-model-len 512  --max-num-seqs 2 --max-num-batched-tokens 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d55fe4",
   "metadata": {},
   "source": [
    "Ë∞ÉÁî®vllmÊé•Âè£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f45fe219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂèØÁî®Ê®°Âûã: ['./sft_qwen3_math_gguf_q8.gguf']\n",
      "ÂõûÂ§ç:  ÂΩìÁÑ∂Ôºå‰ª•‰∏ãÊòØÂæÆÁßØÂàÜ‰∏≠Â∏∏Áî®ÁöÑÂá†‰∏™Âü∫Êú¨ÂØºÊï∞ÂÖ¨ÂºèÔºåÂπ∂ÈôÑ‰∏äÁõ∏Â∫îÁöÑ‰æãÂ≠êÔºö\n",
      "\n",
      "1. Â∏∏Êï∞ÂáΩÊï∞ÁöÑÂØºÊï∞Ôºö\n",
      "   Â¶ÇÊûú \\( f(x) = c \\)ÔºåÂÖ∂‰∏≠ \\( c \\) ÊòØ‰∏Ä‰∏™Â∏∏Êï∞ÔºåÈÇ£‰πà \\( f'(x) = 0 \\)„ÄÇ\n",
      "   ‰æãÂ¶ÇÔºö\\( f(x) = 5 \\)ÔºåÂàô \\( f'(x) = 0 \\)„ÄÇ\n",
      "\n",
      "2. ÂπÇÂáΩÊï∞ÁöÑÂØºÊï∞Ôºö\n",
      "   Â¶ÇÊûú \\( f(x) = x^n \\)ÔºåÂÖ∂‰∏≠ \\( n \\) ÊòØÂÆûÊï∞ÔºåÈÇ£‰πà \\( f'(x) = nx^{n-1} \\)„ÄÇ\n",
      "   ‰æãÂ¶ÇÔºö\\( f(x) = x^3 \\)ÔºåÂàô \\( f'(x) = 3x^2 \\)„ÄÇ\n",
      "\n",
      "3. ÊåáÊï∞ÂáΩÊï∞ÁöÑÂØºÊï∞Ôºö\n",
      "   Â¶ÇÊûú \\( f(x) = e^x \\)ÔºåÈÇ£‰πà \\( f'(x) = e^x \\)„ÄÇ\n",
      "   ‰æãÂ¶ÇÔºö\\( f(x) = e^{2x} \\)ÔºåÂàô \\( f'(x) = 2e^{2x} \\)„ÄÇ\n",
      "\n",
      "4. ÂØπÊï∞ÂáΩÊï∞ÁöÑÂØºÊï∞Ôºö\n",
      "   Â¶ÇÊûú \\( f(x) = \\ln(x) \\)ÔºåÈÇ£‰πà \\( f'(\n",
      "ÂìçÂ∫îÊó∂Èó¥: 3.96Áßí\n",
      "‰ΩøÁî®ÊÉÖÂÜµ: {'completion_tokens': 256, 'prompt_tokens': 23, 'total_tokens': 279, 'completion_tokens_details': None, 'prompt_tokens_details': None}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "class VLLMClient:\n",
    "    def __init__(self, base_url=\"http://localhost:8000/v1\"):\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=\"token-abc123\"  # vLLM ‰∏çÈúÄË¶ÅÁúüÂÆû API key\n",
    "        )\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"ÂàóÂá∫ÂèØÁî®ÁöÑÊ®°Âûã\"\"\"\n",
    "        try:\n",
    "            models = self.client.models.list()\n",
    "            return [model.id for model in models.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Ëé∑ÂèñÊ®°ÂûãÂàóË°®Â§±Ë¥•: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def chat_completion(self, messages, model=None, **kwargs):\n",
    "        \"\"\"ËÅäÂ§©Ë°•ÂÖ®\"\"\"\n",
    "        # Â¶ÇÊûúÊ≤°ÊúâÊåáÂÆöÊ®°ÂûãÔºå‰ΩøÁî®Á¨¨‰∏Ä‰∏™ÂèØÁî®Ê®°Âûã\n",
    "        if not model:\n",
    "            models = self.list_models()\n",
    "            if models:\n",
    "                model = models[0]\n",
    "            else:\n",
    "                raise ValueError(\"Ê≤°ÊúâÂèØÁî®ÁöÑÊ®°Âûã\")\n",
    "        \n",
    "        # ÈªòËÆ§ÂèÇÊï∞\n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 256,\n",
    "            \"stream\": False\n",
    "        }\n",
    "        params.update(kwargs)\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = self.client.chat.completions.create(**params)\n",
    "            end_time = time.time()\n",
    "            \n",
    "            result = {\n",
    "                \"content\": response.choices[0].message.content,\n",
    "                \"usage\": dict(response.usage) if response.usage else None,\n",
    "                \"response_time\": end_time - start_time\n",
    "            }\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"ËÅäÂ§©ËØ∑Ê±ÇÂ§±Ë¥•: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def completion(self, prompt, model=None, **kwargs):\n",
    "        \"\"\"ÊñáÊú¨Ë°•ÂÖ®Ôºà‰º†ÁªüÊñπÂºèÔºâ\"\"\"\n",
    "        if not model:\n",
    "            models = self.list_models()\n",
    "            if models:\n",
    "                model = models[0]\n",
    "        \n",
    "        params = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "        params.update(kwargs)\n",
    "        \n",
    "        try:\n",
    "            response = self.client.completions.create(**params)\n",
    "            return response.choices[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Ë°•ÂÖ®ËØ∑Ê±ÇÂ§±Ë¥•: {e}\")\n",
    "            return None\n",
    "\n",
    "# ‰ΩøÁî®Á§∫‰æã\n",
    "if __name__ == \"__main__\":\n",
    "    vllm_client = VLLMClient()\n",
    "    \n",
    "    # 1. Êü•ÁúãÂèØÁî®Ê®°Âûã\n",
    "    print(\"ÂèØÁî®Ê®°Âûã:\", vllm_client.list_models())\n",
    "    \n",
    "    # 2. ËÅäÂ§©ÂØπËØùÊµãËØï\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"‰Ω†ÊòØ‰∏Ä‰∏™Êï∞Â≠¶ËÄÅÂ∏à„ÄÇ\"},\n",
    "        {\"role\": \"user\", \"content\": \"ÂæÆÁßØÂàÜÂ∏∏Áî®ÂØºÊï∞ÂÖ¨ÂºèÊúâÂì™‰∫õÔºüËØ∑‰∏æ‰æã4‰∏™\"}\n",
    "    ]\n",
    "    \n",
    "    result = vllm_client.chat_completion(messages)\n",
    "    if result:\n",
    "        print(\"ÂõûÂ§ç:\", result[\"content\"])\n",
    "        print(f\"ÂìçÂ∫îÊó∂Èó¥: {result['response_time']:.2f}Áßí\")\n",
    "        if result[\"usage\"]:\n",
    "            print(\"‰ΩøÁî®ÊÉÖÂÜµ:\", result[\"usage\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb51667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"object\":\"list\",\"data\":[{\"id\":\"./sft_qwen3_math_gguf_q8.gguf\",\"object\":\"model\",\"created\":1764505780,\"owned_by\":\"vllm\",\"root\":\"./sft_qwen3_math_gguf_q8.gguf\",\"parent\":null,\"max_model_len\":512,\"permission\":[{\"id\":\"modelperm-224d41fe0500484b872117a8563cc68f\",\"object\":\"model_permission\",\"created\":1764505780,\"allow_create_engine\":false,\"allow_sampling\":true,\"allow_logprobs\":true,\"allow_search_indices\":false,\"allow_view\":true,\"allow_fine_tuning\":false,\"organization\":\"*\",\"group\":null,\"is_blocking\":false}]}]}"
     ]
    }
   ],
   "source": [
    "!curl http://localhost:8000/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d13ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
